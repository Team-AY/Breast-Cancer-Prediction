{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Process \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset and split into Features and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df=pd.read_csv(\"breast-cancer-wisconsin-data/data.csv\")\n",
    "#drop irelevent columns for the classification\n",
    "df = df.drop(columns=['Unnamed: 32', 'id'])\n",
    "# rearange the data for X - featuers and Y leabels \n",
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mapping of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into Train, Test and Valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train_val = X_train_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling the imbalance between the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling imbalance data \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "#the class weight is done only on the train data to impact the learning process and to evaluete beter the model proformence\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as predicted the minorty class 'M'=1 gets higher weight of 1.368"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Init Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.regularizers import l1, l2\n",
    "#from keras.layers import Dropout, Flatten, BatchNormalization\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import callbacks\n",
    "from keras import utils\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(model):\n",
    "    print('================================================================================')\n",
    "    print(f'Evaluation Report for Model: {model.name}')\n",
    "\n",
    "    # accuracy\n",
    "    result = model.evaluate(X_test, y_test, verbose=0)        \n",
    "    print(f'Loss Value: {result[0]:.3f}, Accuracy: {result[1]*100:.3f}%')\n",
    "\n",
    "    # confusion matrix\n",
    "    y_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.round(y_prob).astype(int)    \n",
    "    cm_test = confusion_matrix(y_test, y_pred)    \n",
    "    #sns.heatmap(cm_test,annot=True)\n",
    "    cm_disp = ConfusionMatrixDisplay(cm_test, display_labels=le.classes_)\n",
    "    cm_disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    # classification report\n",
    "    report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "    print(report)\n",
    "\n",
    "    print('Model Summary:')\n",
    "    print(model.summary())\n",
    "    print('================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_history(history):\n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    val_loss_min_pos = np.argmin(history.history['val_loss'])\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', color='#8502d1')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Train and Validation Loss')\n",
    "    plt.plot(val_loss_min_pos, history.history['val_loss'][val_loss_min_pos], 'r*', label='Min Validation Loss')\n",
    "    plt.text(val_loss_min_pos, history.history['val_loss'][val_loss_min_pos], f'({val_loss_min_pos}, {history.history['val_loss'][val_loss_min_pos]:.3f})', va='bottom')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting the training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', color='#8502d1')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.plot(val_loss_min_pos, history.history['val_accuracy'][val_loss_min_pos], 'r*', label='Validation Accuracy @ Min Validation Loss')\n",
    "    plt.text(val_loss_min_pos, history.history['val_accuracy'][val_loss_min_pos], f'({val_loss_min_pos}, {history.history['val_accuracy'][val_loss_min_pos]:.3f})', va='bottom')\n",
    "    plt.title('Train and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_history(history):\n",
    "\n",
    "    val_loss_min_pos = np.argmin(history.history['val_loss'])\n",
    "\n",
    "    train_acc = history.history['accuracy'][val_loss_min_pos]\n",
    "    train_loss = history.history['loss'][val_loss_min_pos]\n",
    "\n",
    "    val_acc = history.history['val_accuracy'][val_loss_min_pos]\n",
    "    val_loss = history.history['val_loss'][val_loss_min_pos]    \n",
    "\n",
    "    val_recall = history.history['val_recall'][val_loss_min_pos]\n",
    "\n",
    "    return {'Train Accuracy': train_acc, 'Train Loss': train_loss, 'Validation Accuracy': val_acc, 'Validation Loss': val_loss, 'Validation Recall': val_recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model, optimizer=optimizers.RMSprop, learning_rate=0.001, epochNum=1000, batchSize=32, en_reduce_lr=False, en_early_stopping=True, pca=False, verbose=\"auto\", Dataset=None):      \n",
    "    \n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001, verbose=0)\n",
    "    checkpoint_filepath = f'model_checkpoints/{model.name}_checkpoint.model.keras'\n",
    "    model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=0)\n",
    "    \n",
    "    callbacks_list = [model_checkpoint_callback]\n",
    "\n",
    "    if en_reduce_lr:\n",
    "        callbacks_list.append(reduce_lr)\n",
    "\n",
    "    if en_early_stopping:\n",
    "        callbacks_list.append(early_stopping)\n",
    "\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy', metrics.Recall(name='recall')])\n",
    "    if Dataset is None:\n",
    "        history = model.fit(X_train, y_train, batch_size=batchSize, epochs=epochNum, validation_data=(X_val, y_val), class_weight=class_weight_dict, callbacks=callbacks_list, verbose=verbose)\n",
    "    else:\n",
    "        history = model.fit(Dataset['X_train'], Dataset['y_train'], batch_size=batchSize, epochs=epochNum, validation_data=(Dataset['X_val'], Dataset['y_val']), class_weight=class_weight_dict, callbacks=callbacks_list, verbose=verbose)\n",
    "\n",
    "    model = models.load_model(checkpoint_filepath)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_kfold(xtrain, ytrain, model_base, optimizer=optimizers.RMSprop,learning_rate=0.001, epochNum=1000, batchSize=32, en_reduce_lr=False, en_early_stopping=True, verbose=\"auto\"):\n",
    "    model = models.clone_model(model_base)\n",
    "    fold_k = StratifiedKFold(n_splits = 5).split(xtrain, ytrain)\n",
    "    \n",
    "    result_df = pd.DataFrame(columns=['k', 'Train Accuracy', 'Train Loss' , 'Validation Accuracy', 'Validation Loss',  'Validation Recall'])\n",
    "    for k , (train, valid) in enumerate(fold_k):\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "\n",
    "        X_train = scaler.fit_transform(xtrain[train])\n",
    "        X_val = scaler.transform(xtrain[valid])\n",
    "        \n",
    "        dataset = {'X_train': X_train, 'y_train': ytrain[train], 'X_val': X_val, 'y_val': ytrain[valid]}\n",
    "        model.set_weights(model_base.get_weights())\n",
    "\n",
    "        \n",
    "\n",
    "        history, model = model_fit(model, optimizer=optimizer, learning_rate=learning_rate, epochNum=epochNum, batchSize=batchSize, en_reduce_lr=en_reduce_lr, en_early_stopping=en_early_stopping, Dataset=dataset, verbose=verbose)    \n",
    "        \n",
    "        proc_data = proc_history(history)\n",
    "\n",
    "        new_row = {'k': k, **proc_data}\n",
    "\n",
    "        result_df.loc[len(result_df)] = new_row\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_MODEL = models.Sequential(name=f'SLP_1')\n",
    "\n",
    "CURRENT_MODEL.add(layers.Input((30,)))\n",
    "CURRENT_MODEL.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.clone_model(CURRENT_MODEL)\n",
    "model.set_weights(CURRENT_MODEL.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline - Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history, model = model_fit(model, epochNum=1000, batchSize=32, verbose=0)\n",
    "\n",
    "proc_data = proc_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(CURRENT_MODEL.get_weights())\n",
    "\n",
    "res_df = model_kfold(X_train_val, y_train_val, model, epochNum=1000, batchSize=32, verbose=0)\n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch_size = pd.DataFrame(columns=['Optimizer', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])\n",
    "\n",
    "for optimizer in [optimizers.Adam, optimizers.RMSprop, optimizers.SGD, optimizers.Adagrad]:\n",
    "    print(f'------------------------------------------------------------------------------------------------------------------')\n",
    "    print(f'Optimizer = {optimizer}')    \n",
    "        \n",
    "    model.set_weights(CURRENT_MODEL.get_weights())   \n",
    "\n",
    "    learning_rate = 0.001\n",
    "    if(optimizer == optimizers.SGD or optimizer==optimizers.Adagrad):\n",
    "        learning_rate = 0.01\n",
    "\n",
    "    history, model = model_fit(model, optimizer=optimizer, learning_rate = learning_rate, epochNum=2000, en_early_stopping=True, verbose=0)        \n",
    "    proc_data = proc_history(history)\n",
    "    \n",
    "    new_row = {'Optimizer': optimizer, **proc_data}\n",
    "    df_batch_size.loc[len(df_batch_size)] = new_row\n",
    "    model_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: We can see in the graphs that the optimizers achieve similar results, while SGD and Adagrad take a large amount of Epochs to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(CURRENT_MODEL.get_weights())\n",
    "ADAM_res_df = model_kfold(X_train_val, y_train_val, model, optimizer=optimizers.Adam, learning_rate=0.001, epochNum=1000, batchSize=32, verbose=0)\n",
    "\n",
    "model.set_weights(CURRENT_MODEL.get_weights())\n",
    "RMS_res_df = model_kfold(X_train_val, y_train_val, model, optimizer=optimizers.RMSprop, learning_rate=0.001, epochNum=1000, batchSize=32, verbose=0)\n",
    "\n",
    "model.set_weights(CURRENT_MODEL.get_weights())\n",
    "SGD_res_df = model_kfold(X_train_val, y_train_val, model, optimizer=optimizers.SGD, learning_rate=0.01, epochNum=1000, batchSize=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Loss Mean: {ADAM_res_df['Validation Loss'].mean()}, Validation Loss STD: {ADAM_res_df['Validation Loss'].std()}\")\n",
    "ADAM_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Loss Mean: {RMS_res_df['Validation Loss'].mean()}, Validation Loss STD: {RMS_res_df['Validation Loss'].std()}\")\n",
    "RMS_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Loss Mean: {SGD_res_df['Validation Loss'].mean()}, Validation Loss STD: {SGD_res_df['Validation Loss'].std()}\")\n",
    "SGD_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: We will take the best Optimizer as SGD, as it yields the lowest STD value in the Validation Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_OPTIMIZER = optimizers.SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning-Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(columns=['Learning Rate', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])\n",
    "\n",
    "for learning_rate in [0.1, 0.01, 0.001, 0.0001]:\n",
    "    print(f'------------------------------------------------------------------------------------------------------------------')\n",
    "    print(f'Learning Rate = {learning_rate}')    \n",
    "        \n",
    "    model.set_weights(CURRENT_MODEL.get_weights())   \n",
    "\n",
    "    history, model = model_fit(model, optimizer=BEST_OPTIMIZER, learning_rate=learning_rate, epochNum=2000, en_early_stopping=True, verbose=0)\n",
    "    proc_data = proc_history(history)\n",
    "    \n",
    "    new_row = {'Learning Rate': learning_rate, **proc_data}\n",
    "    df_res.loc[len(df_res)] = new_row\n",
    "    model_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conlusion: From the graphs above, we can infer that a learning rate of 0.1 and 0.01 yields the same results, however with a learning rate of 0.1 it yields the result 10 times faster, which means the model can handle a large learning rate at the begining of the training. <br>\n",
    "In the next steps, we will include a learning rate scheduler, which will decrease the learning rate once it reaches a plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_arch_df = pd.DataFrame(columns=['Model Name', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x1x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x5x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x10x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x10x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x20x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x20x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluion: We see that NN_30x30x1 is yields the best results for 1st hidden layer. <br>\n",
    "We will check what is the best activation function for this layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x1 - leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30-leaky_relu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='leaky_relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x1 - silu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30-silu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='silu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x1 - elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30-elu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='elu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x1 - tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30-elu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluion: We see that relu yields the best results for 1st hidden layer activation function. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x1x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x5x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x10x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x10x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x20x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x20x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x30x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x30x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluion: We see that NN_30x30x5x1 is yields the best results for 2st hidden layer. <br>\n",
    "We will check what is the best activation function for this layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x5x1 - leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x5-leaky_relu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='leaky_relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x5x1 - silu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x5-silu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='silu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x5x1 - elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x5-elu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='elu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x5x1 - tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x5-tanh-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluion: We see that relu yields the best results for 2st hidden layer activation function. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_arch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_arch_df.sort_values(by='Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: We can see that the Network Architecture of 30x30x5x1 yields the best results, while a leaky-relu or relu activation function in the second hidden layer yields the same results, thus we will remain with the default activation function. <br>\n",
    "We will check this network with KFolds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "res_df = model_kfold(X_train_val, y_train_val, model, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, epochNum=1000, batchSize=32, en_reduce_lr=True, en_early_stopping=True, verbose=0)\n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch-Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch_size = pd.DataFrame(columns=['Batch Size', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])\n",
    "\n",
    "for batch_size in [1, 2, 4, 8, 16, 32, 64, 128]:\n",
    "    print(f'------------------------------------------------------------------------------------------------------------------')\n",
    "    print(f'Batch Size = {batch_size}')    \n",
    "        \n",
    "    model.set_weights(CURRENT_MODEL.get_weights())   \n",
    "\n",
    "    history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=batch_size, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "    proc_data = proc_history(history)\n",
    "    \n",
    "    new_row = {'Batch Size': batch_size, **proc_data}\n",
    "    df_batch_size.loc[len(df_batch_size)] = new_row\n",
    "    model_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in [1, 2, 4, 8, 16, 32, 64, 128]:\n",
    "    model.set_weights(CURRENT_MODEL.get_weights())\n",
    "    batch_size_res_df = model_kfold(X_train_val, y_train_val, model, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, epochNum=1000, batchSize=batch_size, en_reduce_lr=True, en_early_stopping=True, verbose=0)\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    print(f'Batch Size = {batch_size}')\n",
    "    print(f\"Validation Loss Mean: {batch_size_res_df['Validation Loss'].mean()}, Validation Loss STD: {batch_size_res_df['Validation Loss'].std()}\")    \n",
    "    print(batch_size_res_df)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
