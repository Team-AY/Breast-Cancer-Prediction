{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Process \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset and split into Features and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df=pd.read_csv(\"breast-cancer-wisconsin-data/data.csv\")\n",
    "#drop irelevent columns for the classification\n",
    "df = df.drop(columns=['Unnamed: 32', 'id'])\n",
    "# rearange the data for X - featuers and Y leabels \n",
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mapping of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 0, 'M': 1}\n"
     ]
    }
   ],
   "source": [
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into Train, Test and Valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling the imbalance between the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7878787878787878, 1: 1.368421052631579}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#handling imbalance data \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "#the class weight is done only on the train data to impact the learning process and to evaluete beter the model proformence\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as predicted the minorty class 'M'=1 gets higher weight of 1.368"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Init Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.regularizers import l1, l2\n",
    "#from keras.layers import Dropout, Flatten, BatchNormalization\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import callbacks\n",
    "from keras import utils\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(model):\n",
    "    print('================================================================================')\n",
    "    print(f'Evaluation Report for Model: {model.name}')\n",
    "\n",
    "    # accuracy\n",
    "    result = model.evaluate(X_test, y_test, verbose=0)        \n",
    "    print(f'Loss Value: {result[0]:.3f}, Accuracy: {result[1]*100:.3f}%')\n",
    "\n",
    "    # confusion matrix\n",
    "    y_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.round(y_prob).astype(int)    \n",
    "    cm_test = confusion_matrix(y_test, y_pred)    \n",
    "    #sns.heatmap(cm_test,annot=True)\n",
    "    cm_disp = ConfusionMatrixDisplay(cm_test, display_labels=le.classes_)\n",
    "    cm_disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    # classification report\n",
    "    report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "    print(report)\n",
    "\n",
    "    print('Model Summary:')\n",
    "    print(model.summary())\n",
    "    print('================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_history(history):\n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    val_loss_min_pos = np.argmin(history.history['val_loss'])\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', color='#8502d1')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Train and Validation Loss')\n",
    "    plt.plot(val_loss_min_pos, history.history['val_loss'][val_loss_min_pos], 'r*', label='Min Validation Loss')\n",
    "    plt.text(val_loss_min_pos, history.history['val_loss'][val_loss_min_pos], f'({val_loss_min_pos}, {history.history['val_loss'][val_loss_min_pos]:.3f})', va='bottom')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting the training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', color='#8502d1')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.plot(val_loss_min_pos, history.history['val_accuracy'][val_loss_min_pos], 'r*', label='Validation Accuracy @ Min Validation Loss')\n",
    "    plt.text(val_loss_min_pos, history.history['val_accuracy'][val_loss_min_pos], f'({val_loss_min_pos}, {history.history['val_accuracy'][val_loss_min_pos]:.3f})', va='bottom')\n",
    "    plt.title('Train and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_history(history):\n",
    "\n",
    "    val_loss_min_pos = np.argmin(history.history['val_loss'])\n",
    "\n",
    "    train_acc = history.history['accuracy'][val_loss_min_pos]\n",
    "    train_loss = history.history['loss'][val_loss_min_pos]\n",
    "\n",
    "    val_acc = history.history['val_accuracy'][val_loss_min_pos]\n",
    "    val_loss = history.history['val_loss'][val_loss_min_pos]\n",
    "\n",
    "    return {'Train Accuracy': train_acc, 'Train Loss': train_loss, 'Validation Accuracy': val_acc, 'Validation Loss': val_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model, epochNum, pca=False, verbose='auto'):    \n",
    "    utils.set_random_seed(0)\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=verbose)\n",
    "    #reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001, verbose=verbose)\n",
    "    checkpoint_filepath = f'model_checkpoints/{model.name}_checkpoint.model.keras'\n",
    "    model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=verbose)\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=50, epochs=epochNum, validation_data=(X_val, y_val), class_weight=class_weight_dict, callbacks=[model_checkpoint_callback, early_stopping], verbose=verbose)\n",
    "\n",
    "    model = models.load_model(checkpoint_filepath)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Yakir\\Development\\Python\\DL\\Breast-Cancer-Prediction\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "d:\\Yakir\\Development\\Python\\DL\\Breast-Cancer-Prediction\\.venv\\Lib\\site-packages\\keras\\src\\layers\\regularization\\dropout.py:42: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# SLP\n",
    "model = models.Sequential(name=f'SLP_1')\n",
    "model.add(layers.Dense(1, activation='sigmoid', input_shape=(30,)))\n",
    "models_list.append(model)\n",
    "\n",
    "# Add Dropout 0.1, 0.2\n",
    "model = models.Sequential(name=f'SLP_2')\n",
    "model.add(layers.Dropout(rate=0.1, input_shape=(30,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'SLP_3')\n",
    "model.add(layers.Dropout(rate=0.2, input_shape=(30,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 1 Layer\n",
    "model = models.Sequential(name=f'MLP_1')\n",
    "model.add(layers.Dense(2, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_2')\n",
    "model.add(layers.Dense(5, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_3')\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_4')\n",
    "model.add(layers.Dense(15, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_5')\n",
    "model.add(layers.Dense(30, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 2 Layer\n",
    "model = models.Sequential(name=f'MLP_6')\n",
    "model.add(layers.Dense(2, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(2, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid')), models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_7')\n",
    "model.add(layers.Dense(5, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_8')\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_9')\n",
    "model.add(layers.Dense(15, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(15, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_10')\n",
    "model.add(layers.Dense(30, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 3 Layer\n",
    "model = models.Sequential(name=f'MLP_11')\n",
    "model.add(layers.Dense(30, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(15, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_12')\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(15, activation='relu'))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP - Dropout 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 1 Layer\n",
    "model = models.Sequential(name=f'MLP_13')\n",
    "model.add(layers.Dense(2, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_14')\n",
    "model.add(layers.Dense(5, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_15')\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_16')\n",
    "model.add(layers.Dense(15, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_17')\n",
    "model.add(layers.Dense(30, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 2 Layer\n",
    "model = models.Sequential(name=f'MLP_18')\n",
    "model.add(layers.Dense(2, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(2, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid')), models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_19')\n",
    "model.add(layers.Dense(5, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_20')\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_21')\n",
    "model.add(layers.Dense(15, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(15, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_22')\n",
    "model.add(layers.Dense(30, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 3 Layer\n",
    "model = models.Sequential(name=f'MLP_23')\n",
    "model.add(layers.Dense(30, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(15, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_24')\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(15, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP - Dropout 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 1 Layer\n",
    "model = models.Sequential(name=f'MLP_25')\n",
    "model.add(layers.Dense(2, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_26')\n",
    "model.add(layers.Dense(5, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_27')\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_28')\n",
    "model.add(layers.Dense(15, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_29')\n",
    "model.add(layers.Dense(30, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 2 Layer\n",
    "model = models.Sequential(name=f'MLP_30')\n",
    "model.add(layers.Dense(2, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(2, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid')), models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_31')\n",
    "model.add(layers.Dense(5, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_32')\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_33')\n",
    "model.add(layers.Dense(15, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(15, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_34')\n",
    "model.add(layers.Dense(30, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 3 Layer\n",
    "model = models.Sequential(name=f'MLP_35')\n",
    "model.add(layers.Dense(30, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(15, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)\n",
    "\n",
    "model = models.Sequential(name=f'MLP_36')\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(15, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "models_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Sequential name=SLP_1, built=True>,\n",
       " <Sequential name=SLP_2, built=True>,\n",
       " <Sequential name=SLP_3, built=True>,\n",
       " <Sequential name=MLP_1, built=True>,\n",
       " <Sequential name=MLP_2, built=True>,\n",
       " <Sequential name=MLP_3, built=True>,\n",
       " <Sequential name=MLP_4, built=True>,\n",
       " <Sequential name=MLP_5, built=True>,\n",
       " <Sequential name=MLP_6, built=True>,\n",
       " <Sequential name=MLP_7, built=True>,\n",
       " <Sequential name=MLP_8, built=True>,\n",
       " <Sequential name=MLP_9, built=True>,\n",
       " <Sequential name=MLP_10, built=True>,\n",
       " <Sequential name=MLP_11, built=True>,\n",
       " <Sequential name=MLP_12, built=True>,\n",
       " <Sequential name=MLP_13, built=True>,\n",
       " <Sequential name=MLP_14, built=True>,\n",
       " <Sequential name=MLP_15, built=True>,\n",
       " <Sequential name=MLP_16, built=True>,\n",
       " <Sequential name=MLP_17, built=True>,\n",
       " <Sequential name=MLP_18, built=True>,\n",
       " <Sequential name=MLP_19, built=True>,\n",
       " <Sequential name=MLP_20, built=True>,\n",
       " <Sequential name=MLP_21, built=True>,\n",
       " <Sequential name=MLP_22, built=True>,\n",
       " <Sequential name=MLP_23, built=True>,\n",
       " <Sequential name=MLP_24, built=True>,\n",
       " <Sequential name=MLP_25, built=True>,\n",
       " <Sequential name=MLP_26, built=True>,\n",
       " <Sequential name=MLP_27, built=True>,\n",
       " <Sequential name=MLP_28, built=True>,\n",
       " <Sequential name=MLP_29, built=True>,\n",
       " <Sequential name=MLP_30, built=True>,\n",
       " <Sequential name=MLP_31, built=True>,\n",
       " <Sequential name=MLP_32, built=True>,\n",
       " <Sequential name=MLP_33, built=True>,\n",
       " <Sequential name=MLP_34, built=True>,\n",
       " <Sequential name=MLP_35, built=True>,\n",
       " <Sequential name=MLP_36, built=True>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Models:\n",
      "Fitting SLP_1... Done! (2.7189948558807373 [s])\n",
      "Fitting SLP_2... Done! (3.126000165939331 [s])\n",
      "Fitting SLP_3... Done! (3.1361300945281982 [s])\n",
      "Fitting MLP_1... Done! (2.856057643890381 [s])\n",
      "Fitting MLP_2... Done! (2.770138740539551 [s])\n",
      "Fitting MLP_3... Done! (2.8081390857696533 [s])\n",
      "Fitting MLP_4... Done! (2.8282053470611572 [s])\n",
      "Fitting MLP_5... Done! (2.81964373588562 [s])\n",
      "Fitting MLP_6... Done! (2.997732639312744 [s])\n",
      "Fitting MLP_7... Done! (2.9592044353485107 [s])\n",
      "Fitting MLP_8... Done! (2.9713380336761475 [s])\n",
      "Fitting MLP_9... Done! (3.035731315612793 [s])\n",
      "Fitting MLP_10... Done! (2.98618483543396 [s])\n",
      "Fitting MLP_11... Done! (3.198922872543335 [s])\n",
      "Fitting MLP_12... Done! (3.2087631225585938 [s])\n",
      "Fitting MLP_13... Done! (3.0255067348480225 [s])\n",
      "Fitting MLP_14... Done! (2.851266622543335 [s])\n",
      "Fitting MLP_15... Done! (3.021934747695923 [s])\n",
      "Fitting MLP_16... Done! (2.8589487075805664 [s])\n",
      "Fitting MLP_17... Done! (2.9198501110076904 [s])\n",
      "Fitting MLP_18... Done! (3.260495662689209 [s])\n",
      "Fitting MLP_19... Done! (3.2443692684173584 [s])\n",
      "Fitting MLP_20... Done! (3.125725030899048 [s])\n",
      "Fitting MLP_21... Done! (4.020143985748291 [s])\n",
      "Fitting MLP_22... Done! (3.1194629669189453 [s])\n",
      "Fitting MLP_23... Done! (3.3781027793884277 [s])\n",
      "Fitting MLP_24... Done! (3.338914155960083 [s])\n",
      "Fitting MLP_25... Done! (2.907982110977173 [s])\n",
      "Fitting MLP_26... Done! (3.579557180404663 [s])\n",
      "Fitting MLP_27... Done! (2.8425419330596924 [s])\n",
      "Fitting MLP_28... Done! (2.864161968231201 [s])\n",
      "Fitting MLP_29... Done! (2.9272804260253906 [s])\n",
      "Fitting MLP_30... Done! (20.477577686309814 [s])\n",
      "Fitting MLP_31... Done! (3.191446542739868 [s])\n",
      "Fitting MLP_32... Done! (3.0842716693878174 [s])\n",
      "Fitting MLP_33... Done! (3.1131300926208496 [s])\n",
      "Fitting MLP_34... Done! (3.7267847061157227 [s])\n",
      "Fitting MLP_35... Done! (3.355271816253662 [s])\n",
      "Fitting MLP_36... Done! (4.307246208190918 [s])\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "models_df = pd.DataFrame(columns=['Model Name', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Test Accuracy', 'Test Loss'])\n",
    "print('Processing Models:')\n",
    "for model in models_list:\n",
    "    print(f'Fitting {model.name}...', end= \" \")    \n",
    "\n",
    "    t_start = time()    \n",
    "    history, model = model_fit(model, 2000, verbose=0)\n",
    "    t_end = time()\n",
    "\n",
    "    proc_data = proc_history(history)  \n",
    "\n",
    "    eval_res = model.evaluate(X_test, y_test, verbose=0)  \n",
    "\n",
    "    new_row = {'Model Name': model.name, **proc_data, 'Test Accuracy': eval_res[1], 'Test Loss': eval_res[0]}\n",
    "    models_df.loc[len(models_df)] = new_row\n",
    "\n",
    "    print(f'Done! ({t_end-t_start} [s])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SLP_1</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.057050</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.130379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SLP_2</td>\n",
       "      <td>0.975275</td>\n",
       "      <td>0.076719</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.127834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLP_3</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.098340</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.129408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP_1</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.063895</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.151033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP_2</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.063604</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.129547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP_3</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.056463</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.133248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP_4</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.049312</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.125480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP_5</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.055378</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.119391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP_6</td>\n",
       "      <td>0.991758</td>\n",
       "      <td>0.100684</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.234191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP_7</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.063793</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.119832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP_8</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.076697</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.131215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLP_9</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.089312</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.143639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLP_10</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.041954</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.107870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP_11</td>\n",
       "      <td>0.972527</td>\n",
       "      <td>0.057731</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.124360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP_12</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.062383</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.134825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP_13</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.200141</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.134228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP_14</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.169580</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.222316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP_15</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.077093</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.150881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP_16</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.061854</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.112554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLP_17</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.063976</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.121711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLP_18</td>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.356862</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.119138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MLP_19</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.119822</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.179404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MLP_20</td>\n",
       "      <td>0.936813</td>\n",
       "      <td>0.177223</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.138055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MLP_21</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.114968</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.132155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MLP_22</td>\n",
       "      <td>0.975275</td>\n",
       "      <td>0.083180</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.121616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MLP_23</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.129410</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.135031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MLP_24</td>\n",
       "      <td>0.939560</td>\n",
       "      <td>0.153448</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.142123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MLP_25</td>\n",
       "      <td>0.854396</td>\n",
       "      <td>0.313131</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MLP_26</td>\n",
       "      <td>0.898352</td>\n",
       "      <td>0.291979</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.273913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MLP_27</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.111542</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.137395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MLP_28</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.097536</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.118470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MLP_29</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.105314</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.120299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MLP_30</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.559131</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.286214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MLP_31</td>\n",
       "      <td>0.843407</td>\n",
       "      <td>0.360838</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.319194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MLP_32</td>\n",
       "      <td>0.947802</td>\n",
       "      <td>0.167961</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.121722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MLP_33</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.180564</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.131295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MLP_34</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.105285</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.133853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>MLP_35</td>\n",
       "      <td>0.925824</td>\n",
       "      <td>0.197864</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.149835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MLP_36</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.137440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Name  Train Accuracy  Train Loss  Validation Accuracy  \\\n",
       "0       SLP_1        0.989011    0.057050             0.967033   \n",
       "1       SLP_2        0.975275    0.076719             0.967033   \n",
       "2       SLP_3        0.969780    0.098340             0.967033   \n",
       "3       MLP_1        0.978022    0.063895             0.967033   \n",
       "4       MLP_2        0.983516    0.063604             0.956044   \n",
       "5       MLP_3        0.980769    0.056463             0.956044   \n",
       "6       MLP_4        0.986264    0.049312             0.967033   \n",
       "7       MLP_5        0.983516    0.055378             0.967033   \n",
       "8       MLP_6        0.991758    0.100684             0.956044   \n",
       "9       MLP_7        0.980769    0.063793             0.967033   \n",
       "10      MLP_8        0.969780    0.076697             0.956044   \n",
       "11      MLP_9        0.967033    0.089312             0.967033   \n",
       "12     MLP_10        0.986264    0.041954             0.956044   \n",
       "13     MLP_11        0.972527    0.057731             0.967033   \n",
       "14     MLP_12        0.978022    0.062383             0.967033   \n",
       "15     MLP_13        0.903846    0.200141             0.956044   \n",
       "16     MLP_14        0.967033    0.169580             0.967033   \n",
       "17     MLP_15        0.969780    0.077093             0.956044   \n",
       "18     MLP_16        0.978022    0.061854             0.967033   \n",
       "19     MLP_17        0.983516    0.063976             0.967033   \n",
       "20     MLP_18        0.802198    0.356862             0.967033   \n",
       "21     MLP_19        0.986264    0.119822             0.967033   \n",
       "22     MLP_20        0.936813    0.177223             0.956044   \n",
       "23     MLP_21        0.967033    0.114968             0.956044   \n",
       "24     MLP_22        0.975275    0.083180             0.967033   \n",
       "25     MLP_23        0.961538    0.129410             0.956044   \n",
       "26     MLP_24        0.939560    0.153448             0.967033   \n",
       "27     MLP_25        0.854396    0.313131             0.967033   \n",
       "28     MLP_26        0.898352    0.291979             0.967033   \n",
       "29     MLP_27        0.964286    0.111542             0.956044   \n",
       "30     MLP_28        0.969780    0.097536             0.967033   \n",
       "31     MLP_29        0.956044    0.105314             0.967033   \n",
       "32     MLP_30        0.747253    0.559131             0.967033   \n",
       "33     MLP_31        0.843407    0.360838             0.967033   \n",
       "34     MLP_32        0.947802    0.167961             0.967033   \n",
       "35     MLP_33        0.942308    0.180564             0.956044   \n",
       "36     MLP_34        0.964286    0.105285             0.967033   \n",
       "37     MLP_35        0.925824    0.197864             0.934066   \n",
       "38     MLP_36        0.912088    0.204013             0.945055   \n",
       "\n",
       "    Validation Loss  \n",
       "0          0.130379  \n",
       "1          0.127834  \n",
       "2          0.129408  \n",
       "3          0.151033  \n",
       "4          0.129547  \n",
       "5          0.133248  \n",
       "6          0.125480  \n",
       "7          0.119391  \n",
       "8          0.234191  \n",
       "9          0.119832  \n",
       "10         0.131215  \n",
       "11         0.143639  \n",
       "12         0.107870  \n",
       "13         0.124360  \n",
       "14         0.134825  \n",
       "15         0.134228  \n",
       "16         0.222316  \n",
       "17         0.150881  \n",
       "18         0.112554  \n",
       "19         0.121711  \n",
       "20         0.119138  \n",
       "21         0.179404  \n",
       "22         0.138055  \n",
       "23         0.132155  \n",
       "24         0.121616  \n",
       "25         0.135031  \n",
       "26         0.142123  \n",
       "27         0.235400  \n",
       "28         0.273913  \n",
       "29         0.137395  \n",
       "30         0.118470  \n",
       "31         0.120299  \n",
       "32         0.286214  \n",
       "33         0.319194  \n",
       "34         0.121722  \n",
       "35         0.131295  \n",
       "36         0.133853  \n",
       "37         0.149835  \n",
       "38         0.137440  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP_16</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.061854</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.112554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MLP_28</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.097536</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.118470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLP_18</td>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.356862</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.119138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP_5</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.055378</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.119391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP_7</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.063793</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.119832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Name  Train Accuracy  Train Loss  Validation Accuracy  \\\n",
       "18     MLP_16        0.978022    0.061854             0.967033   \n",
       "30     MLP_28        0.969780    0.097536             0.967033   \n",
       "20     MLP_18        0.802198    0.356862             0.967033   \n",
       "7       MLP_5        0.983516    0.055378             0.967033   \n",
       "9       MLP_7        0.980769    0.063793             0.967033   \n",
       "\n",
       "    Validation Loss  \n",
       "18         0.112554  \n",
       "30         0.118470  \n",
       "20         0.119138  \n",
       "7          0.119391  \n",
       "9          0.119832  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df.sort_values(ascending=[False, True], by=['Validation Accuracy', 'Validation Loss']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SLP_1</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.063783</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.124730</td>\n",
       "      <td>0.078172</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SLP_2</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.077067</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.127795</td>\n",
       "      <td>0.086853</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLP_3</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.092649</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.132687</td>\n",
       "      <td>0.082823</td>\n",
       "      <td>0.95614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Name  Train Accuracy  Train Loss  Validation Accuracy  \\\n",
       "0      SLP_1        0.989011    0.063783             0.967033   \n",
       "1      SLP_2        0.980769    0.077067             0.967033   \n",
       "2      SLP_3        0.961538    0.092649             0.967033   \n",
       "\n",
       "   Validation Loss  Test Loss  Test Accuracy  \n",
       "0         0.124730   0.078172        0.95614  \n",
       "1         0.127795   0.086853        0.95614  \n",
       "2         0.132687   0.082823        0.95614  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SLP_1</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.063556</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.125773</td>\n",
       "      <td>0.95614</td>\n",
       "      <td>0.077795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SLP_2</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.082634</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.124347</td>\n",
       "      <td>0.95614</td>\n",
       "      <td>0.076808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLP_3</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.084145</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.126178</td>\n",
       "      <td>0.95614</td>\n",
       "      <td>0.072279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Name  Train Accuracy  Train Loss  Validation Accuracy  \\\n",
       "0      SLP_1        0.989011    0.063556             0.967033   \n",
       "1      SLP_2        0.969780    0.082634             0.967033   \n",
       "2      SLP_3        0.980769    0.084145             0.967033   \n",
       "\n",
       "   Validation Loss  Test Accuracy  Test Loss  \n",
       "0         0.125773        0.95614   0.077795  \n",
       "1         0.124347        0.95614   0.076808  \n",
       "2         0.126178        0.95614   0.072279  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>MLP_35</td>\n",
       "      <td>0.936813</td>\n",
       "      <td>0.213505</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.115124</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.090839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP_14</td>\n",
       "      <td>0.947802</td>\n",
       "      <td>0.122453</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.117187</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.072930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP_16</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.068708</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.120926</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.088397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLP_9</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.050299</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.121068</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.069267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP_2</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.047493</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.123979</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.076535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP_13</td>\n",
       "      <td>0.876374</td>\n",
       "      <td>0.236691</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.124363</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.073229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MLP_26</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.141056</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.127005</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.087613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SLP_1</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.062885</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.127054</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.075304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP_12</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.051441</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.127158</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.070626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MLP_33</td>\n",
       "      <td>0.920330</td>\n",
       "      <td>0.160369</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.130773</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.088441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLP_3</td>\n",
       "      <td>0.953297</td>\n",
       "      <td>0.111513</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.132281</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.087407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SLP_2</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.087710</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.135163</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.086997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP_15</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.083142</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.083680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP_5</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.061677</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.137732</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.087010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MLP_24</td>\n",
       "      <td>0.939560</td>\n",
       "      <td>0.165762</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.138537</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.068869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP_11</td>\n",
       "      <td>0.975275</td>\n",
       "      <td>0.063885</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.139266</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.081908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MLP_19</td>\n",
       "      <td>0.901099</td>\n",
       "      <td>0.235424</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.141136</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.080527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP_6</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.062386</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.145922</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.064732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MLP_34</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.137769</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.146597</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.074993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP_7</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.067543</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.149452</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.088618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MLP_31</td>\n",
       "      <td>0.887363</td>\n",
       "      <td>0.305434</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.151435</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.092112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLP_10</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.069918</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.113776</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.074543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP_8</td>\n",
       "      <td>0.975275</td>\n",
       "      <td>0.069972</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.119801</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.072247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP_4</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.044345</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.120779</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.067147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MLP_21</td>\n",
       "      <td>0.947802</td>\n",
       "      <td>0.127483</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.121845</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.085003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MLP_27</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.098026</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.125262</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.080395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLP_17</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.061692</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.126677</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.069598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP_3</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.070783</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.104709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MLP_29</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.071776</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.127632</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.067740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP_1</td>\n",
       "      <td>0.994505</td>\n",
       "      <td>0.052433</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.131203</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.103604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MLP_22</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.133471</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.085524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MLP_28</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.101229</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.142179</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.082931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MLP_23</td>\n",
       "      <td>0.947802</td>\n",
       "      <td>0.148740</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.146806</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.094226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MLP_20</td>\n",
       "      <td>0.947802</td>\n",
       "      <td>0.151480</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.148370</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.092928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MLP_36</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.177596</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.154935</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.101945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MLP_32</td>\n",
       "      <td>0.925824</td>\n",
       "      <td>0.262092</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.249525</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.126714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MLP_25</td>\n",
       "      <td>0.813187</td>\n",
       "      <td>0.356778</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.238595</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.175749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLP_18</td>\n",
       "      <td>0.733516</td>\n",
       "      <td>0.473009</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.254151</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.209381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MLP_30</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.694054</td>\n",
       "      <td>0.604396</td>\n",
       "      <td>0.685160</td>\n",
       "      <td>0.622807</td>\n",
       "      <td>0.683592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Name  Train Accuracy  Train Loss  Validation Accuracy  \\\n",
       "37     MLP_35        0.936813    0.213505             0.967033   \n",
       "16     MLP_14        0.947802    0.122453             0.967033   \n",
       "18     MLP_16        0.980769    0.068708             0.967033   \n",
       "11      MLP_9        0.986264    0.050299             0.967033   \n",
       "4       MLP_2        0.986264    0.047493             0.967033   \n",
       "15     MLP_13        0.876374    0.236691             0.967033   \n",
       "28     MLP_26        0.964286    0.141056             0.967033   \n",
       "0       SLP_1        0.983516    0.062885             0.967033   \n",
       "14     MLP_12        0.980769    0.051441             0.967033   \n",
       "35     MLP_33        0.920330    0.160369             0.967033   \n",
       "2       SLP_3        0.953297    0.111513             0.967033   \n",
       "1       SLP_2        0.969780    0.087710             0.967033   \n",
       "17     MLP_15        0.969780    0.083142             0.967033   \n",
       "7       MLP_5        0.983516    0.061677             0.967033   \n",
       "26     MLP_24        0.939560    0.165762             0.967033   \n",
       "13     MLP_11        0.975275    0.063885             0.967033   \n",
       "21     MLP_19        0.901099    0.235424             0.967033   \n",
       "8       MLP_6        0.978022    0.062386             0.967033   \n",
       "36     MLP_34        0.945055    0.137769             0.967033   \n",
       "9       MLP_7        0.980769    0.067543             0.967033   \n",
       "33     MLP_31        0.887363    0.305434             0.967033   \n",
       "12     MLP_10        0.978022    0.069918             0.956044   \n",
       "10      MLP_8        0.975275    0.069972             0.956044   \n",
       "6       MLP_4        0.989011    0.044345             0.956044   \n",
       "23     MLP_21        0.947802    0.127483             0.956044   \n",
       "29     MLP_27        0.967033    0.098026             0.956044   \n",
       "19     MLP_17        0.986264    0.061692             0.956044   \n",
       "5       MLP_3        0.986264    0.070783             0.956044   \n",
       "31     MLP_29        0.969780    0.071776             0.956044   \n",
       "3       MLP_1        0.994505    0.052433             0.956044   \n",
       "24     MLP_22        0.967033    0.099542             0.956044   \n",
       "30     MLP_28        0.969780    0.101229             0.956044   \n",
       "25     MLP_23        0.947802    0.148740             0.956044   \n",
       "22     MLP_20        0.947802    0.151480             0.956044   \n",
       "38     MLP_36        0.956044    0.177596             0.956044   \n",
       "34     MLP_32        0.925824    0.262092             0.956044   \n",
       "27     MLP_25        0.813187    0.356778             0.945055   \n",
       "20     MLP_18        0.733516    0.473009             0.945055   \n",
       "32     MLP_30        0.634615    0.694054             0.604396   \n",
       "\n",
       "    Validation Loss  Test Accuracy  Test Loss  \n",
       "37         0.115124       0.964912   0.090839  \n",
       "16         0.117187       0.973684   0.072930  \n",
       "18         0.120926       0.956140   0.088397  \n",
       "11         0.121068       0.973684   0.069267  \n",
       "4          0.123979       0.973684   0.076535  \n",
       "15         0.124363       0.973684   0.073229  \n",
       "28         0.127005       0.956140   0.087613  \n",
       "0          0.127054       0.964912   0.075304  \n",
       "14         0.127158       0.947368   0.070626  \n",
       "35         0.130773       0.964912   0.088441  \n",
       "2          0.132281       0.964912   0.087407  \n",
       "1          0.135163       0.964912   0.086997  \n",
       "17         0.136200       0.964912   0.083680  \n",
       "7          0.137732       0.956140   0.087010  \n",
       "26         0.138537       0.973684   0.068869  \n",
       "13         0.139266       0.964912   0.081908  \n",
       "21         0.141136       0.956140   0.080527  \n",
       "8          0.145922       0.964912   0.064732  \n",
       "36         0.146597       0.964912   0.074993  \n",
       "9          0.149452       0.964912   0.088618  \n",
       "33         0.151435       0.964912   0.092112  \n",
       "12         0.113776       0.956140   0.074543  \n",
       "10         0.119801       0.973684   0.072247  \n",
       "6          0.120779       0.964912   0.067147  \n",
       "23         0.121845       0.964912   0.085003  \n",
       "29         0.125262       0.956140   0.080395  \n",
       "19         0.126677       0.964912   0.069598  \n",
       "5          0.127273       0.964912   0.104709  \n",
       "31         0.127632       0.964912   0.067740  \n",
       "3          0.131203       0.956140   0.103604  \n",
       "24         0.133471       0.956140   0.085524  \n",
       "30         0.142179       0.956140   0.082931  \n",
       "25         0.146806       0.956140   0.094226  \n",
       "22         0.148370       0.956140   0.092928  \n",
       "38         0.154935       0.973684   0.101945  \n",
       "34         0.249525       0.964912   0.126714  \n",
       "27         0.238595       0.982456   0.175749  \n",
       "20         0.254151       0.956140   0.209381  \n",
       "32         0.685160       0.622807   0.683592  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df.sort_values(ascending=[False, True], by=['Validation Accuracy', 'Validation Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MLP_25</td>\n",
       "      <td>0.813187</td>\n",
       "      <td>0.356778</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.238595</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.175749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MLP_36</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.177596</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.154935</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.101945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP_13</td>\n",
       "      <td>0.876374</td>\n",
       "      <td>0.236691</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.124363</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.073229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MLP_24</td>\n",
       "      <td>0.939560</td>\n",
       "      <td>0.165762</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.138537</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.068869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLP_9</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.050299</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.121068</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.069267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP_8</td>\n",
       "      <td>0.975275</td>\n",
       "      <td>0.069972</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.119801</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.072247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP_14</td>\n",
       "      <td>0.947802</td>\n",
       "      <td>0.122453</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.117187</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.072930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP_2</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.047493</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.123979</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.076535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MLP_31</td>\n",
       "      <td>0.887363</td>\n",
       "      <td>0.305434</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.151435</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.092112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP_15</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.083142</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.083680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MLP_32</td>\n",
       "      <td>0.925824</td>\n",
       "      <td>0.262092</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.249525</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.126714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MLP_33</td>\n",
       "      <td>0.920330</td>\n",
       "      <td>0.160369</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.130773</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.088441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MLP_34</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.137769</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.146597</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.074993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MLP_21</td>\n",
       "      <td>0.947802</td>\n",
       "      <td>0.127483</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.121845</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.085003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SLP_2</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.087710</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.135163</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.086997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>MLP_35</td>\n",
       "      <td>0.936813</td>\n",
       "      <td>0.213505</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.115124</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.090839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MLP_29</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.071776</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.127632</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.067740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SLP_1</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.062885</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.127054</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.075304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLP_17</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.061692</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.126677</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.069598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLP_3</td>\n",
       "      <td>0.953297</td>\n",
       "      <td>0.111513</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.132281</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.087407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP_3</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.070783</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.104709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP_11</td>\n",
       "      <td>0.975275</td>\n",
       "      <td>0.063885</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.139266</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.081908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP_4</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.044345</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.120779</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.067147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP_6</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.062386</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.145922</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.064732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP_7</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.067543</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.149452</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.088618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MLP_27</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.098026</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.125262</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.080395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP_1</td>\n",
       "      <td>0.994505</td>\n",
       "      <td>0.052433</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.131203</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.103604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP_5</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.061677</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.137732</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.087010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MLP_28</td>\n",
       "      <td>0.969780</td>\n",
       "      <td>0.101229</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.142179</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.082931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP_16</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.068708</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.120926</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.088397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLP_10</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.069918</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.113776</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.074543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MLP_23</td>\n",
       "      <td>0.947802</td>\n",
       "      <td>0.148740</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.146806</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.094226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MLP_22</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.133471</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.085524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MLP_20</td>\n",
       "      <td>0.947802</td>\n",
       "      <td>0.151480</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.148370</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.092928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MLP_19</td>\n",
       "      <td>0.901099</td>\n",
       "      <td>0.235424</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.141136</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.080527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLP_18</td>\n",
       "      <td>0.733516</td>\n",
       "      <td>0.473009</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.254151</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.209381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MLP_26</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.141056</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.127005</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.087613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP_12</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.051441</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.127158</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.070626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MLP_30</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.694054</td>\n",
       "      <td>0.604396</td>\n",
       "      <td>0.685160</td>\n",
       "      <td>0.622807</td>\n",
       "      <td>0.683592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Name  Train Accuracy  Train Loss  Validation Accuracy  \\\n",
       "27     MLP_25        0.813187    0.356778             0.945055   \n",
       "38     MLP_36        0.956044    0.177596             0.956044   \n",
       "15     MLP_13        0.876374    0.236691             0.967033   \n",
       "26     MLP_24        0.939560    0.165762             0.967033   \n",
       "11      MLP_9        0.986264    0.050299             0.967033   \n",
       "10      MLP_8        0.975275    0.069972             0.956044   \n",
       "16     MLP_14        0.947802    0.122453             0.967033   \n",
       "4       MLP_2        0.986264    0.047493             0.967033   \n",
       "33     MLP_31        0.887363    0.305434             0.967033   \n",
       "17     MLP_15        0.969780    0.083142             0.967033   \n",
       "34     MLP_32        0.925824    0.262092             0.956044   \n",
       "35     MLP_33        0.920330    0.160369             0.967033   \n",
       "36     MLP_34        0.945055    0.137769             0.967033   \n",
       "23     MLP_21        0.947802    0.127483             0.956044   \n",
       "1       SLP_2        0.969780    0.087710             0.967033   \n",
       "37     MLP_35        0.936813    0.213505             0.967033   \n",
       "31     MLP_29        0.969780    0.071776             0.956044   \n",
       "0       SLP_1        0.983516    0.062885             0.967033   \n",
       "19     MLP_17        0.986264    0.061692             0.956044   \n",
       "2       SLP_3        0.953297    0.111513             0.967033   \n",
       "5       MLP_3        0.986264    0.070783             0.956044   \n",
       "13     MLP_11        0.975275    0.063885             0.967033   \n",
       "6       MLP_4        0.989011    0.044345             0.956044   \n",
       "8       MLP_6        0.978022    0.062386             0.967033   \n",
       "9       MLP_7        0.980769    0.067543             0.967033   \n",
       "29     MLP_27        0.967033    0.098026             0.956044   \n",
       "3       MLP_1        0.994505    0.052433             0.956044   \n",
       "7       MLP_5        0.983516    0.061677             0.967033   \n",
       "30     MLP_28        0.969780    0.101229             0.956044   \n",
       "18     MLP_16        0.980769    0.068708             0.967033   \n",
       "12     MLP_10        0.978022    0.069918             0.956044   \n",
       "25     MLP_23        0.947802    0.148740             0.956044   \n",
       "24     MLP_22        0.967033    0.099542             0.956044   \n",
       "22     MLP_20        0.947802    0.151480             0.956044   \n",
       "21     MLP_19        0.901099    0.235424             0.967033   \n",
       "20     MLP_18        0.733516    0.473009             0.945055   \n",
       "28     MLP_26        0.964286    0.141056             0.967033   \n",
       "14     MLP_12        0.980769    0.051441             0.967033   \n",
       "32     MLP_30        0.634615    0.694054             0.604396   \n",
       "\n",
       "    Validation Loss  Test Accuracy  Test Loss  \n",
       "27         0.238595       0.982456   0.175749  \n",
       "38         0.154935       0.973684   0.101945  \n",
       "15         0.124363       0.973684   0.073229  \n",
       "26         0.138537       0.973684   0.068869  \n",
       "11         0.121068       0.973684   0.069267  \n",
       "10         0.119801       0.973684   0.072247  \n",
       "16         0.117187       0.973684   0.072930  \n",
       "4          0.123979       0.973684   0.076535  \n",
       "33         0.151435       0.964912   0.092112  \n",
       "17         0.136200       0.964912   0.083680  \n",
       "34         0.249525       0.964912   0.126714  \n",
       "35         0.130773       0.964912   0.088441  \n",
       "36         0.146597       0.964912   0.074993  \n",
       "23         0.121845       0.964912   0.085003  \n",
       "1          0.135163       0.964912   0.086997  \n",
       "37         0.115124       0.964912   0.090839  \n",
       "31         0.127632       0.964912   0.067740  \n",
       "0          0.127054       0.964912   0.075304  \n",
       "19         0.126677       0.964912   0.069598  \n",
       "2          0.132281       0.964912   0.087407  \n",
       "5          0.127273       0.964912   0.104709  \n",
       "13         0.139266       0.964912   0.081908  \n",
       "6          0.120779       0.964912   0.067147  \n",
       "8          0.145922       0.964912   0.064732  \n",
       "9          0.149452       0.964912   0.088618  \n",
       "29         0.125262       0.956140   0.080395  \n",
       "3          0.131203       0.956140   0.103604  \n",
       "7          0.137732       0.956140   0.087010  \n",
       "30         0.142179       0.956140   0.082931  \n",
       "18         0.120926       0.956140   0.088397  \n",
       "12         0.113776       0.956140   0.074543  \n",
       "25         0.146806       0.956140   0.094226  \n",
       "24         0.133471       0.956140   0.085524  \n",
       "22         0.148370       0.956140   0.092928  \n",
       "21         0.141136       0.956140   0.080527  \n",
       "20         0.254151       0.956140   0.209381  \n",
       "28         0.127005       0.956140   0.087613  \n",
       "14         0.127158       0.947368   0.070626  \n",
       "32         0.685160       0.622807   0.683592  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df.sort_values(ascending=False, by='Test Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SLP_1</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.060272</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.127216</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.073434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SLP_2</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.070067</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.127630</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.073539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLP_3</td>\n",
       "      <td>0.972527</td>\n",
       "      <td>0.079117</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.124747</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.068795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP_1</td>\n",
       "      <td>0.994505</td>\n",
       "      <td>0.037767</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.126939</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.091871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP_2</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.047196</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.124598</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.076577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP_3</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.067250</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.127990</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.102415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP_4</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.038758</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.122287</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.064709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP_5</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.060775</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.139149</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.086962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP_6</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.061742</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.146914</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.063948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP_7</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.066981</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.150776</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.087910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP_8</td>\n",
       "      <td>0.975275</td>\n",
       "      <td>0.068807</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.121821</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.072150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLP_9</td>\n",
       "      <td>0.986264</td>\n",
       "      <td>0.049255</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.123651</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.068560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLP_10</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.066739</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.116659</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.072461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP_11</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.145122</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.082289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP_12</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.049749</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.130533</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.069522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP_13</td>\n",
       "      <td>0.887363</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.124146</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.072429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP_14</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.135704</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.118273</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.072583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP_15</td>\n",
       "      <td>0.975275</td>\n",
       "      <td>0.070149</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.136145</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.081461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP_16</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.061625</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.121927</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.087696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLP_17</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.064412</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.127647</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.067393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLP_18</td>\n",
       "      <td>0.708791</td>\n",
       "      <td>0.480057</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.250421</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.201227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MLP_19</td>\n",
       "      <td>0.898352</td>\n",
       "      <td>0.223897</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.141318</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.080029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MLP_20</td>\n",
       "      <td>0.958791</td>\n",
       "      <td>0.119918</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.150445</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.081226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MLP_21</td>\n",
       "      <td>0.958791</td>\n",
       "      <td>0.113801</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.121144</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.081875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MLP_22</td>\n",
       "      <td>0.953297</td>\n",
       "      <td>0.117279</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.139092</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.086887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MLP_23</td>\n",
       "      <td>0.953297</td>\n",
       "      <td>0.140144</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.147750</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.090553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MLP_24</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.137033</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.140359</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.067610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MLP_25</td>\n",
       "      <td>0.829670</td>\n",
       "      <td>0.348097</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.237557</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.172815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MLP_26</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.156999</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.124551</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.083118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MLP_27</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.118887</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.126763</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.080138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MLP_28</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.093827</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.141623</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.075469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MLP_29</td>\n",
       "      <td>0.972527</td>\n",
       "      <td>0.077789</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.129972</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.067659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MLP_30</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.694106</td>\n",
       "      <td>0.604396</td>\n",
       "      <td>0.684974</td>\n",
       "      <td>0.622807</td>\n",
       "      <td>0.683364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MLP_31</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.289643</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.152019</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.081775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MLP_32</td>\n",
       "      <td>0.914835</td>\n",
       "      <td>0.255582</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.252039</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.124477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MLP_33</td>\n",
       "      <td>0.909341</td>\n",
       "      <td>0.196055</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.132883</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.088083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MLP_34</td>\n",
       "      <td>0.947802</td>\n",
       "      <td>0.151893</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.149426</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.072249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>MLP_35</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.236159</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.116741</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.090363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MLP_36</td>\n",
       "      <td>0.958791</td>\n",
       "      <td>0.186930</td>\n",
       "      <td>0.956044</td>\n",
       "      <td>0.160653</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.102902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Name  Train Accuracy  Train Loss  Validation Accuracy  \\\n",
       "0       SLP_1        0.983516    0.060272             0.967033   \n",
       "1       SLP_2        0.986264    0.070067             0.967033   \n",
       "2       SLP_3        0.972527    0.079117             0.967033   \n",
       "3       MLP_1        0.994505    0.037767             0.956044   \n",
       "4       MLP_2        0.986264    0.047196             0.967033   \n",
       "5       MLP_3        0.986264    0.067250             0.956044   \n",
       "6       MLP_4        0.989011    0.038758             0.956044   \n",
       "7       MLP_5        0.983516    0.060775             0.967033   \n",
       "8       MLP_6        0.978022    0.061742             0.967033   \n",
       "9       MLP_7        0.980769    0.066981             0.967033   \n",
       "10      MLP_8        0.975275    0.068807             0.956044   \n",
       "11      MLP_9        0.986264    0.049255             0.967033   \n",
       "12     MLP_10        0.978022    0.066739             0.967033   \n",
       "13     MLP_11        0.978022    0.060741             0.967033   \n",
       "14     MLP_12        0.983516    0.049749             0.967033   \n",
       "15     MLP_13        0.887363    0.228100             0.967033   \n",
       "16     MLP_14        0.934066    0.135704             0.967033   \n",
       "17     MLP_15        0.975275    0.070149             0.967033   \n",
       "18     MLP_16        0.978022    0.061625             0.967033   \n",
       "19     MLP_17        0.983516    0.064412             0.956044   \n",
       "20     MLP_18        0.708791    0.480057             0.945055   \n",
       "21     MLP_19        0.898352    0.223897             0.967033   \n",
       "22     MLP_20        0.958791    0.119918             0.967033   \n",
       "23     MLP_21        0.958791    0.113801             0.956044   \n",
       "24     MLP_22        0.953297    0.117279             0.956044   \n",
       "25     MLP_23        0.953297    0.140144             0.956044   \n",
       "26     MLP_24        0.945055    0.137033             0.967033   \n",
       "27     MLP_25        0.829670    0.348097             0.945055   \n",
       "28     MLP_26        0.956044    0.156999             0.967033   \n",
       "29     MLP_27        0.945055    0.118887             0.956044   \n",
       "30     MLP_28        0.956044    0.093827             0.956044   \n",
       "31     MLP_29        0.972527    0.077789             0.956044   \n",
       "32     MLP_30        0.634615    0.694106             0.604396   \n",
       "33     MLP_31        0.890110    0.289643             0.967033   \n",
       "34     MLP_32        0.914835    0.255582             0.956044   \n",
       "35     MLP_33        0.909341    0.196055             0.967033   \n",
       "36     MLP_34        0.947802    0.151893             0.967033   \n",
       "37     MLP_35        0.934066    0.236159             0.967033   \n",
       "38     MLP_36        0.958791    0.186930             0.956044   \n",
       "\n",
       "    Validation Loss  Test Accuracy  Test Loss  \n",
       "0          0.127216       0.964912   0.073434  \n",
       "1          0.127630       0.973684   0.073539  \n",
       "2          0.124747       0.964912   0.068795  \n",
       "3          0.126939       0.956140   0.091871  \n",
       "4          0.124598       0.973684   0.076577  \n",
       "5          0.127990       0.964912   0.102415  \n",
       "6          0.122287       0.964912   0.064709  \n",
       "7          0.139149       0.956140   0.086962  \n",
       "8          0.146914       0.964912   0.063948  \n",
       "9          0.150776       0.956140   0.087910  \n",
       "10         0.121821       0.973684   0.072150  \n",
       "11         0.123651       0.973684   0.068560  \n",
       "12         0.116659       0.956140   0.072461  \n",
       "13         0.145122       0.964912   0.082289  \n",
       "14         0.130533       0.947368   0.069522  \n",
       "15         0.124146       0.973684   0.072429  \n",
       "16         0.118273       0.973684   0.072583  \n",
       "17         0.136145       0.964912   0.081461  \n",
       "18         0.121927       0.956140   0.087696  \n",
       "19         0.127647       0.964912   0.067393  \n",
       "20         0.250421       0.964912   0.201227  \n",
       "21         0.141318       0.956140   0.080029  \n",
       "22         0.150445       0.956140   0.081226  \n",
       "23         0.121144       0.964912   0.081875  \n",
       "24         0.139092       0.956140   0.086887  \n",
       "25         0.147750       0.956140   0.090553  \n",
       "26         0.140359       0.973684   0.067610  \n",
       "27         0.237557       0.982456   0.172815  \n",
       "28         0.124551       0.956140   0.083118  \n",
       "29         0.126763       0.956140   0.080138  \n",
       "30         0.141623       0.956140   0.075469  \n",
       "31         0.129972       0.964912   0.067659  \n",
       "32         0.684974       0.622807   0.683364  \n",
       "33         0.152019       0.964912   0.081775  \n",
       "34         0.252039       0.964912   0.124477  \n",
       "35         0.132883       0.964912   0.088083  \n",
       "36         0.149426       0.964912   0.072249  \n",
       "37         0.116741       0.964912   0.090363  \n",
       "38         0.160653       0.973684   0.102902  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
