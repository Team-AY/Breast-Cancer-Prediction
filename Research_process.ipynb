{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Process \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset and split into Features and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df=pd.read_csv(\"breast-cancer-wisconsin-data/data.csv\")\n",
    "#drop irelevent columns for the classification\n",
    "df = df.drop(columns=['Unnamed: 32', 'id'])\n",
    "# rearange the data for X - featuers and Y leabels \n",
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mapping of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into Train, Test and Valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "XData_train_val, XData_test, ydata_train_val, ydata_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "XData_train, XData_val, ydata_train, ydata_val = train_test_split(XData_train_val, ydata_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(XData_train)\n",
    "X_test = scaler.transform(XData_test)\n",
    "X_val = scaler.transform(XData_val)\n",
    "\n",
    "X_train_val = XData_train_val.values\n",
    "\n",
    "y_train = ydata_train\n",
    "y_test = ydata_test\n",
    "y_val = ydata_val\n",
    "\n",
    "y_train_val = ydata_train_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling the imbalance between the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling imbalance data \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "#the class weight is done only on the train data to impact the learning process and to evaluete beter the model proformence\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as predicted the minorty class 'M'=1 gets higher weight of 1.368"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Init Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.regularizers import l1, l2\n",
    "#from keras.layers import Dropout, Flatten, BatchNormalization\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import callbacks\n",
    "from keras import utils\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(model):\n",
    "    print('================================================================================')\n",
    "    print(f'Evaluation Report for Model: {model.name}')\n",
    "\n",
    "    # accuracy\n",
    "    result = model.evaluate(X_test, y_test, verbose=0)        \n",
    "    print(f'Loss Value: {result[0]:.3f}, Accuracy: {result[1]*100:.3f}%')\n",
    "\n",
    "    # confusion matrix\n",
    "    y_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.round(y_prob).astype(int)    \n",
    "    cm_test = confusion_matrix(y_test, y_pred)    \n",
    "    #sns.heatmap(cm_test,annot=True)\n",
    "    cm_disp = ConfusionMatrixDisplay(cm_test, display_labels=le.classes_)\n",
    "    cm_disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    # classification report\n",
    "    report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "    print(report)\n",
    "\n",
    "    print('Model Summary:')\n",
    "    print(model.summary())\n",
    "    print('================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_history(history, param=None):\n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    val_loss_min_pos = np.argmin(history.history['val_loss'])\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', color='#8502d1')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "    if param is None:\n",
    "        plt.title('Train and Validation Loss')\n",
    "    else:\n",
    "        plt.title(f'Train and Validation Loss on {param}')\n",
    "\n",
    "    plt.plot(val_loss_min_pos, history.history['val_loss'][val_loss_min_pos], 'r*', label='Min Validation Loss')\n",
    "    plt.text(val_loss_min_pos, history.history['val_loss'][val_loss_min_pos], f'({val_loss_min_pos}, {history.history['val_loss'][val_loss_min_pos]:.3f})', va='bottom')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting the training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', color='#8502d1')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.plot(val_loss_min_pos, history.history['val_accuracy'][val_loss_min_pos], 'r*', label='Validation Accuracy @ Min Validation Loss')\n",
    "    plt.text(val_loss_min_pos, history.history['val_accuracy'][val_loss_min_pos], f'({val_loss_min_pos}, {history.history['val_accuracy'][val_loss_min_pos]:.3f})', va='bottom')\n",
    "\n",
    "    if param is None:\n",
    "        plt.title('Train and Validation Accuracy')\n",
    "    else:\n",
    "        plt.title(f'Train and Validation Accuracy on {param}')\n",
    "        \n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_history(history):\n",
    "\n",
    "    val_loss_min_pos = np.argmin(history.history['val_loss'])\n",
    "\n",
    "    train_acc = history.history['accuracy'][val_loss_min_pos]\n",
    "    train_loss = history.history['loss'][val_loss_min_pos]\n",
    "\n",
    "    val_acc = history.history['val_accuracy'][val_loss_min_pos]\n",
    "    val_loss = history.history['val_loss'][val_loss_min_pos]    \n",
    "\n",
    "    val_recall = history.history['val_recall'][val_loss_min_pos]\n",
    "\n",
    "    return {'Train Accuracy': train_acc, 'Train Loss': train_loss, 'Validation Accuracy': val_acc, 'Validation Loss': val_loss, 'Validation Recall': val_recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model, optimizer=optimizers.RMSprop, learning_rate=0.001, epochNum=1000, batchSize=32, en_reduce_lr=False, en_early_stopping=True, pca=False, verbose=\"auto\", Dataset=None):      \n",
    "    \n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001, verbose=0)\n",
    "    checkpoint_filepath = f'model_checkpoints/{model.name}_checkpoint.model.keras'\n",
    "    model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=0)\n",
    "    \n",
    "    callbacks_list = [model_checkpoint_callback]\n",
    "\n",
    "    if en_reduce_lr:\n",
    "        callbacks_list.append(reduce_lr)\n",
    "\n",
    "    if en_early_stopping:\n",
    "        callbacks_list.append(early_stopping)\n",
    "\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy', metrics.Recall(name='recall')])\n",
    "    if Dataset is None:\n",
    "        history = model.fit(X_train, y_train, batch_size=batchSize, epochs=epochNum, validation_data=(X_val, y_val), class_weight=class_weight_dict, callbacks=callbacks_list, verbose=verbose)\n",
    "    else:\n",
    "        history = model.fit(Dataset['X_train'], Dataset['y_train'], batch_size=batchSize, epochs=epochNum, validation_data=(Dataset['X_val'], Dataset['y_val']), class_weight=class_weight_dict, callbacks=callbacks_list, verbose=verbose)\n",
    "\n",
    "    model = models.load_model(checkpoint_filepath)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_kfold(xtrain, ytrain, model_base, optimizer=optimizers.RMSprop,learning_rate=0.001, epochNum=1000, batchSize=32, en_reduce_lr=False, en_early_stopping=True, verbose=\"auto\"):\n",
    "    model = models.clone_model(model_base)\n",
    "    fold_k = StratifiedKFold(n_splits = 5).split(xtrain, ytrain)\n",
    "    \n",
    "    result_df = pd.DataFrame(columns=['k', 'Train Accuracy', 'Train Loss' , 'Validation Accuracy', 'Validation Loss',  'Validation Recall'])\n",
    "    for k , (train, valid) in enumerate(fold_k):\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "\n",
    "        X_train = scaler.fit_transform(xtrain[train])\n",
    "        X_val = scaler.transform(xtrain[valid])\n",
    "        \n",
    "        dataset = {'X_train': X_train, 'y_train': ytrain[train], 'X_val': X_val, 'y_val': ytrain[valid]}\n",
    "        model.set_weights(model_base.get_weights())\n",
    "\n",
    "        \n",
    "\n",
    "        history, model = model_fit(model, optimizer=optimizer, learning_rate=learning_rate, epochNum=epochNum, batchSize=batchSize, en_reduce_lr=en_reduce_lr, en_early_stopping=en_early_stopping, Dataset=dataset, verbose=verbose)    \n",
    "        \n",
    "        proc_data = proc_history(history)\n",
    "\n",
    "        new_row = {'k': k, **proc_data}\n",
    "\n",
    "        result_df.loc[len(result_df)] = new_row\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_MODEL = models.Sequential(name=f'SLP_1')\n",
    "\n",
    "CURRENT_MODEL.add(layers.Input((30,)))\n",
    "CURRENT_MODEL.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.clone_model(CURRENT_MODEL)\n",
    "model.set_weights(CURRENT_MODEL.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline - Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history, model = model_fit(model, epochNum=1000, batchSize=32, verbose=0)\n",
    "\n",
    "proc_data = proc_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(CURRENT_MODEL.get_weights())\n",
    "\n",
    "res_df = model_kfold(X_train_val, y_train_val, model, epochNum=1000, batchSize=32, verbose=0)\n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimizer = pd.DataFrame(columns=['Optimizer', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])\n",
    "\n",
    "for optimizer in [optimizers.Adam, optimizers.RMSprop, optimizers.SGD, optimizers.Adagrad]:\n",
    "    print(f'------------------------------------------------------------------------------------------------------------------')\n",
    "    print(f'Optimizer = {optimizer}')    \n",
    "        \n",
    "    model.set_weights(CURRENT_MODEL.get_weights())   \n",
    "\n",
    "    learning_rate = 0.001\n",
    "    if(optimizer == optimizers.SGD or optimizer==optimizers.Adagrad):\n",
    "        learning_rate = 0.01\n",
    "\n",
    "    history, model = model_fit(model, optimizer=optimizer, learning_rate = learning_rate, epochNum=2000, en_early_stopping=True, verbose=0)        \n",
    "    proc_data = proc_history(history)\n",
    "    \n",
    "    new_row = {'Optimizer': optimizer, **proc_data}\n",
    "    df_optimizer.loc[len(df_optimizer)] = new_row\n",
    "    model_history(history, f'Optimizer = {optimizer.__name__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: We can see in the graphs that the optimizers achieve similar results, while SGD and Adagrad take a large amount of Epochs to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(CURRENT_MODEL.get_weights())\n",
    "ADAM_res_df = model_kfold(X_train_val, y_train_val, model, optimizer=optimizers.Adam, learning_rate=0.001, epochNum=1000, batchSize=32, verbose=0)\n",
    "\n",
    "model.set_weights(CURRENT_MODEL.get_weights())\n",
    "RMS_res_df = model_kfold(X_train_val, y_train_val, model, optimizer=optimizers.RMSprop, learning_rate=0.001, epochNum=1000, batchSize=32, verbose=0)\n",
    "\n",
    "model.set_weights(CURRENT_MODEL.get_weights())\n",
    "SGD_res_df = model_kfold(X_train_val, y_train_val, model, optimizer=optimizers.SGD, learning_rate=0.01, epochNum=1000, batchSize=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Loss Mean: {ADAM_res_df['Validation Loss'].mean()}, Validation Loss STD: {ADAM_res_df['Validation Loss'].std()}\")\n",
    "ADAM_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Loss Mean: {RMS_res_df['Validation Loss'].mean()}, Validation Loss STD: {RMS_res_df['Validation Loss'].std()}\")\n",
    "RMS_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Loss Mean: {SGD_res_df['Validation Loss'].mean()}, Validation Loss STD: {SGD_res_df['Validation Loss'].std()}\")\n",
    "SGD_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: We will take the best Optimizer as SGD, as it yields the lowest STD value in the Validation Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_OPTIMIZER = optimizers.SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning-Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(columns=['Learning Rate', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])\n",
    "\n",
    "for learning_rate in [0.1, 0.01, 0.001, 0.0001]:\n",
    "    print(f'------------------------------------------------------------------------------------------------------------------')\n",
    "    print(f'Learning Rate = {learning_rate}')    \n",
    "        \n",
    "    model.set_weights(CURRENT_MODEL.get_weights())   \n",
    "\n",
    "    history, model = model_fit(model, optimizer=BEST_OPTIMIZER, learning_rate=learning_rate, epochNum=2000, en_early_stopping=True, verbose=0)\n",
    "    proc_data = proc_history(history)\n",
    "    \n",
    "    new_row = {'Learning Rate': learning_rate, **proc_data}\n",
    "    df_res.loc[len(df_res)] = new_row\n",
    "    model_history(history, f'Learning Rate = {learning_rate}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conlusion: From the graphs above, we can infer that a learning rate of 0.1 and 0.01 yields the same results, however with a learning rate of 0.1 it yields the result 10 times faster, which means the model can handle a large learning rate at the begining of the training. <br>\n",
    "In the next steps, we will include a learning rate scheduler, which will decrease the learning rate once it reaches a plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_arch_df = pd.DataFrame(columns=['Model Name', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x1x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x5x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x10x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x10x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x20x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x20x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluion: We see that NN_30x30x1 is yields the best results for 1st hidden layer. <br>\n",
    "We will check what is the best activation function for this layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x1 - leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30-leaky_relu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='leaky_relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x1 - silu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30-silu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='silu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x1 - elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30-elu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='elu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x1 - tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30-elu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluion: We see that relu yields the best results for 1st hidden layer activation function. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x1x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x5x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x10x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x10x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x20x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x20x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x30x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x30x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluion: We see that NN_30x30x5x1 is yields the best results for 2st hidden layer. <br>\n",
    "We will check what is the best activation function for this layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x5x1 - leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x5-leaky_relu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='leaky_relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x5x1 - silu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x5-silu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='silu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x5x1 - elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x5-elu-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='elu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN 30x30x5x1 - tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x5-tanh-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_arch_df.loc[len(nn_arch_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluion: We see that relu yields the best results for 2st hidden layer activation function. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_arch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_arch_df.sort_values(by='Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: We can see that the Network Architecture of 30x30x5x1 yields the best results, while a leaky-relu or relu activation function in the second hidden layer yields the same results, thus we will remain with the default activation function. <br>\n",
    "We will check this network with KFolds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "        \n",
    "model = models.Sequential(name=f'NN_30x30x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "res_df = model_kfold(X_train_val, y_train_val, model, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, epochNum=1000, batchSize=32, en_reduce_lr=True, en_early_stopping=True, verbose=0)\n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Conclusion:</u></b> We can see that the 30x30x5x1 is the Network Architecture that yields the best and stable results. <br>\n",
    "With that said, we will also still take the SLP, which is the basic Network Architecture (No hidden layers) into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "BEST_MODEL = models.Sequential(name=f'NN_30x30x5x1')\n",
    "\n",
    "BEST_MODEL.add(layers.Input((30,)))\n",
    "BEST_MODEL.add(layers.Dense(30, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dense(5, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch-Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch_size = pd.DataFrame(columns=['Batch Size', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])\n",
    "\n",
    "model = models.clone_model(BEST_MODEL)\n",
    "\n",
    "for batch_size in [1, 2, 4, 8, 16, 32, 64, 128]:\n",
    "    print(f'------------------------------------------------------------------------------------------------------------------')\n",
    "    print(f'Batch Size = {batch_size}')    \n",
    "        \n",
    "    model.set_weights(BEST_MODEL.get_weights())   \n",
    "\n",
    "    history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=batch_size, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "    proc_data = proc_history(history)\n",
    "    \n",
    "    new_row = {'Batch Size': batch_size, **proc_data}\n",
    "    df_batch_size.loc[len(df_batch_size)] = new_row\n",
    "    model_history(history, f'Batch Size = {batch_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in [1, 2, 4, 8, 16, 32, 64, 128]:\n",
    "    model.set_weights(BEST_MODEL.get_weights())\n",
    "    batch_size_res_df = model_kfold(X_train_val, y_train_val, model, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, epochNum=1000, batchSize=batch_size, en_reduce_lr=True, en_early_stopping=True, verbose=0)\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    print(f'Batch Size = {batch_size}')\n",
    "    print(f\"Validation Loss Mean: {batch_size_res_df['Validation Loss'].mean()}, Validation Loss STD: {batch_size_res_df['Validation Loss'].std()}\")    \n",
    "    print(f\"Validation Recall Mean: {batch_size_res_df['Validation Recall'].mean()}, Validation Recall STD: {batch_size_res_df['Validation Recall'].std()}\")    \n",
    "    print(batch_size_res_df)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: As we can see from the results, BATCH_SIZE = 8 yielded the best results in the Validation Loss (mean), <br> \n",
    "As well as over the recall metric in the 5 folds. Thus, we will choose BATCH_SIZE = 8 as the best BATCH_SIZE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_drop_df = pd.DataFrame(columns=['Model Name', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Layer Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30-D2-x30x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_drop_df.loc[len(nn_drop_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Layer Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30-D3-x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_drop_df.loc[len(nn_drop_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Layer Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5-D3-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_drop_df.loc[len(nn_drop_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First & Second Layer Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30-D3-x5-D3-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_drop_df.loc[len(nn_drop_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFold Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_drop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input Layer Dropout Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30-D2-x30x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "res_df = model_kfold(X_train_val, y_train_val, model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_reduce_lr=True, en_early_stopping=True, verbose=0)\n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Recall Mean: {res_df['Validation Recall'].mean()}, Validation Recall STD: {res_df['Validation Recall'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Layer Dropout Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30-D3-x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "res_df = model_kfold(X_train_val, y_train_val, model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_reduce_lr=True, en_early_stopping=True, verbose=0)\n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Recall Mean: {res_df['Validation Recall'].mean()}, Validation Recall STD: {res_df['Validation Recall'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second Layer Dropout Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5-D3-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "res_df = model_kfold(X_train_val, y_train_val, model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_reduce_lr=True, en_early_stopping=True, verbose=0)\n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Recall Mean: {res_df['Validation Recall'].mean()}, Validation Recall STD: {res_df['Validation Recall'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First & Second Layer Dropout Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30-D3-x5-D3-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "res_df = model_kfold(X_train_val, y_train_val, model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_reduce_lr=True, en_early_stopping=True, verbose=0)\n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Recall Mean: {res_df['Validation Recall'].mean()}, Validation Recall STD: {res_df['Validation Recall'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune Dropout Rate = [0.1, 0.2, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "for rate in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "\n",
    "    print('-----------------------------------------------------------------------------')\n",
    "    print(f'Dropout Rate = {rate}')\n",
    "\n",
    "    model = models.Sequential(name=f'NN_30x30x5-D{int(rate*10)}-x1')\n",
    "\n",
    "    model.add(layers.Input((30,)))\n",
    "    model.add(layers.Dense(30, activation='relu'))\n",
    "    model.add(layers.Dense(5, activation='relu'))\n",
    "    model.add(layers.Dropout(rate))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    res_df = model_kfold(X_train_val, y_train_val, model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_reduce_lr=True, en_early_stopping=True, verbose=0)\n",
    "    print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "    print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "    print(f\"Validation Recall Mean: {res_df['Validation Recall'].mean()}, Validation Recall STD: {res_df['Validation Recall'].std()}\")    \n",
    "    print(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5-D1-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_drop_df.loc[len(nn_drop_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: We chose the best Dropout Addition to the Network as a Dropout to the Second Layer only with a rate of 0.1. <br>\n",
    "Thus, we will update our BEST_MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "BEST_MODEL = models.Sequential(name=f'NN_30x30x5-D1-x1')\n",
    "\n",
    "BEST_MODEL.add(layers.Input((30,)))\n",
    "BEST_MODEL.add(layers.Dense(30, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dense(5, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dropout(0.1))\n",
    "BEST_MODEL.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_l2_df = pd.DataFrame(columns=['Model Name', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Layer L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30_L2_0001x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.L2(0.0001)))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_l2_df.loc[len(nn_l2_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Layer L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5_L2_0001x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu', kernel_regularizer=regularizers.L2(0.0001)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_l2_df.loc[len(nn_l2_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First & Second Layer L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30_L2_0001x5_L2_0001x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.L2(0.0001)))\n",
    "model.add(layers.Dense(5, activation='relu', kernel_regularizer=regularizers.L2(0.0001)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_l2_df.loc[len(nn_l2_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_l2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: As we can see, adding the Regularization L2 didn't improve our model performance. <br>\n",
    "Now we will check how L1 Regularization affects our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_l1_df = pd.DataFrame(columns=['Model Name', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Layer L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30_L1_0001x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.L1(0.0001)))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_l1_df.loc[len(nn_l1_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Layer L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5_L1_0001x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu', kernel_regularizer=regularizers.L1(0.0001)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_l1_df.loc[len(nn_l1_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First & Second Layer L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30_L2_0001x5_L1_0001x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.L1(0.0001)))\n",
    "model.add(layers.Dense(5, activation='relu', kernel_regularizer=regularizers.L1(0.0001)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_l1_df.loc[len(nn_l1_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_l1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: As we can see, adding the Regularization L1 also didn't improve our model performance. <br>\n",
    "Thus, we will not use L2 nor L1 Regularizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "BEST_MODEL = models.Sequential(name=f'NN_30x30x5-D1-x1')\n",
    "\n",
    "BEST_MODEL.add(layers.Input((30,)))\n",
    "BEST_MODEL.add(layers.Dense(30, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dense(5, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dropout(0.1))\n",
    "BEST_MODEL.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_bottleneck_df = pd.DataFrame(columns=['Model Name', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottleneck between Input and First Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x1x30x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_bottleneck_df.loc[len(nn_bottleneck_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottleneck between First and Second Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x3x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_bottleneck_df.loc[len(nn_bottleneck_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x1x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_bottleneck_df.loc[len(nn_bottleneck_df)] = new_row\n",
    "\n",
    "model_history(history, f'Model Name = {model.name}')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottleneck as First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x1x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_bottleneck_df.loc[len(nn_bottleneck_df)] = new_row\n",
    "\n",
    "model_history(history)\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_bottleneck_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFold Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x1x5x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "res_df = model_kfold(X_train_val, y_train_val, model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_reduce_lr=True, en_early_stopping=True, verbose=0)\n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Recall Mean: {res_df['Validation Recall'].mean()}, Validation Recall STD: {res_df['Validation Recall'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: We will not continue with bottleneck when we check other parameters, <br>\n",
    "However, we will try to see the results of this model on the X_test dataset lateron, because we see that this model could have potential due to its stable graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "BEST_MODEL = models.Sequential(name=f'NN_30x30x5-D1-x1')\n",
    "\n",
    "BEST_MODEL.add(layers.Input((30,)))\n",
    "BEST_MODEL.add(layers.Dense(30, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dense(5, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dropout(0.1))\n",
    "BEST_MODEL.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_batchnorm_df = pd.DataFrame(columns=['Model Name', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Layer Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5-D1-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_batchnorm_df.loc[len(nn_batchnorm_df)] = new_row\n",
    "\n",
    "model_history(history, f'Batch Normalization = Second Layer')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Layer Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5-D1-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Model Name': model.name, **proc_data}\n",
    "nn_batchnorm_df.loc[len(nn_batchnorm_df)] = new_row\n",
    "\n",
    "model_history(history, f'Batch Normalization = First Layer')\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Layer KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5-D1-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "res_df = model_kfold(X_train_val, y_train_val, model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_reduce_lr=True, en_early_stopping=True, verbose=0)\n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Recall Mean: {res_df['Validation Recall'].mean()}, Validation Recall STD: {res_df['Validation Recall'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Layer KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5-D1-x1')\n",
    "\n",
    "model.add(layers.Input((30,)))\n",
    "model.add(layers.Dense(30))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "res_df = model_kfold(X_train_val, y_train_val, model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_reduce_lr=True, en_early_stopping=True, verbose=0)\n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Recall Mean: {res_df['Validation Recall'].mean()}, Validation Recall STD: {res_df['Validation Recall'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: This model didn't yield a higher validation score than our best model. <br>\n",
    "For the rest of our parameters search, we will keep the previous best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "BEST_MODEL = models.Sequential(name=f'NN_30x30x5-D1-x1')\n",
    "\n",
    "BEST_MODEL.add(layers.Input((30,)))\n",
    "BEST_MODEL.add(layers.Dense(30, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dense(5, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dropout(0.1))\n",
    "BEST_MODEL.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pca_df = pd.DataFrame(columns=['PCA Components', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca_5 = decomposition.PCA(n_components=5)\n",
    "pca_10 = decomposition.PCA(n_components=10)\n",
    "pca_15 = decomposition.PCA(n_components=15)\n",
    "pca_20 = decomposition.PCA(n_components=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA N_Components=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5x1')\n",
    "\n",
    "X_train_pca_5 = pca_5.fit_transform(XData_train)\n",
    "X_test_pca_5 = pca_5.transform(XData_test)\n",
    "X_val_pca_5 = pca_5.transform(XData_val)\n",
    "\n",
    "model.add(layers.Input((X_train_pca_5.shape[1],)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "scaler_pca_5 = RobustScaler()\n",
    "X_train_pca_5 = scaler_pca_5.fit_transform(X_train_pca_5)\n",
    "X_test_pca_5 = scaler_pca_5.transform(X_test_pca_5)\n",
    "X_val_pca_5 = scaler_pca_5.transform(X_val_pca_5)\n",
    "\n",
    "dataset = {'X_train': X_train_pca_5, 'y_train': y_train, 'X_val': X_val_pca_5, 'y_val': y_val}\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0, Dataset=dataset)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'PCA Components': 5, **proc_data}\n",
    "nn_pca_df.loc[len(nn_pca_df)] = new_row\n",
    "\n",
    "model_history(history, f'PCA Components = 5')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA N_Components=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5x1')\n",
    "\n",
    "X_train_pca_10 = pca_10.fit_transform(XData_train)\n",
    "X_test_pca_10 = pca_10.transform(XData_test)\n",
    "X_val_pca_10 = pca_10.transform(XData_val)\n",
    "\n",
    "model.add(layers.Input((X_train_pca_10.shape[1],)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "scaler_pca_10 = RobustScaler()\n",
    "X_train_pca_10 = scaler_pca_10.fit_transform(X_train_pca_10)\n",
    "X_test_pca_10 = scaler_pca_10.transform(X_test_pca_10)\n",
    "X_val_pca_10 = scaler_pca_10.transform(X_val_pca_10)\n",
    "\n",
    "dataset = {'X_train': X_train_pca_10, 'y_train': y_train, 'X_val': X_val_pca_10, 'y_val': y_val}\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0, Dataset=dataset)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'PCA Components': 10, **proc_data}\n",
    "nn_pca_df.loc[len(nn_pca_df)] = new_row\n",
    "\n",
    "model_history(history, f'PCA Components = 10')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA N_Components=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5x1')\n",
    "\n",
    "X_train_pca_15 = pca_15.fit_transform(XData_train)\n",
    "X_test_pca_15 = pca_15.transform(XData_test)\n",
    "X_val_pca_15 = pca_15.transform(XData_val)\n",
    "\n",
    "model.add(layers.Input((X_train_pca_15.shape[1],)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "scaler_pca_15 = RobustScaler()\n",
    "X_train_pca_15 = scaler_pca_15.fit_transform(X_train_pca_15)\n",
    "X_test_pca_15 = scaler_pca_15.transform(X_test_pca_15)\n",
    "X_val_pca_15 = scaler_pca_15.transform(X_val_pca_15)\n",
    "\n",
    "dataset = {'X_train': X_train_pca_15, 'y_train': y_train, 'X_val': X_val_pca_15, 'y_val': y_val}\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0, Dataset=dataset)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'PCA Components': 15, **proc_data}\n",
    "nn_pca_df.loc[len(nn_pca_df)] = new_row\n",
    "\n",
    "model_history(history, f'PCA Components = 15')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA N_Components=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5x1')\n",
    "\n",
    "X_train_pca_20 = pca_20.fit_transform(XData_train)\n",
    "X_test_pca_20 = pca_20.transform(XData_test)\n",
    "X_val_pca_20 = pca_20.transform(XData_val)\n",
    "\n",
    "model.add(layers.Input((X_train_pca_20.shape[1],)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "scaler_pca_20 = RobustScaler()\n",
    "X_train_pca_20 = scaler_pca_20.fit_transform(X_train_pca_20)\n",
    "X_test_pca_20 = scaler_pca_20.transform(X_test_pca_20)\n",
    "X_val_pca_20 = scaler_pca_20.transform(X_val_pca_20)\n",
    "\n",
    "dataset = {'X_train': X_train_pca_20, 'y_train': y_train, 'X_val': X_val_pca_20, 'y_val': y_val}\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0, Dataset=dataset)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'PCA Components': 20, **proc_data}\n",
    "nn_pca_df.loc[len(nn_pca_df)] = new_row\n",
    "\n",
    "model_history(history, f'PCA Components = 20')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: In our case, using PCA didn't help our model to learn better. <br>\n",
    "We do see that even with fewer features, the model can achieve good enough results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_corr_df = pd.DataFrame(columns=['Threshold', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in our EDA, there are some features that correlates to each other. Therefore, we can remove them in order to reduce the dimensionality of the problem and to make the model learn more easily. <br>\n",
    "We will check how it affects the performance of our best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = XData_train.corr()\n",
    "colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "plt.figure(figsize=(14,14))\n",
    "sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},\n",
    "            cmap = colormap, linewidths=0.1, linecolor='white')\n",
    "plt.title('Correlation of Breast Cancer Dataset Features', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are some correlated features in our dataset. <br>\n",
    "We will evaluate how many features we remove for different thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove features that are above the defined threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_columns(X, threshold):\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = X.corr()\n",
    "\n",
    "    # Create a mask to identify upper triangle of the correlation matrix\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find index of features with correlation greater than the threshold\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]    \n",
    "                                                                \n",
    "    # Drop the highly correlated features\n",
    "    X_reduced = X.drop(columns=to_drop)    \n",
    "\n",
    "    return {'number of features to drop': len(to_drop), 'to_drop': to_drop, 'reduced': list(X_reduced.columns)}\n",
    "\n",
    "df_corr_comp = pd.DataFrame(columns=['number of features to drop', 'to_drop', 'reduced'])\n",
    "df_corr_comp.index.name = 'threshold'\n",
    "\n",
    "thresholds = [0.85, 0.9, 0.95]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    new_row = get_corr_columns(X, threshold)\n",
    "    df_corr_comp.loc[threshold] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to check different high values of correlation in order to see how it influences the dimensons of the features. <br>\n",
    "We assume that correlation value bigger than 0.9 means a high enough correlation for us to drop the feature. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Threshold = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5-D1-x1')\n",
    "\n",
    "X_train_085 = XData_train.drop(columns=df_corr_comp.loc[0.85]['to_drop'])\n",
    "X_test_085 = XData_test.drop(columns=df_corr_comp.loc[0.85]['to_drop'])\n",
    "X_val_085 = XData_val.drop(columns=df_corr_comp.loc[0.85]['to_drop'])\n",
    "\n",
    "model.add(layers.Input((X_train_085.shape[1],)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "scaler_085 = RobustScaler()\n",
    "X_train_085 = scaler_085.fit_transform(X_train_085)\n",
    "X_test_085 = scaler_085.transform(X_test_085)\n",
    "X_val_085 = scaler_085.transform(X_val_085)\n",
    "\n",
    "dataset = {'X_train': X_train_085, 'y_train': y_train, 'X_val': X_val_085, 'y_val': y_val}\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0, Dataset=dataset)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Threshold': 0.85, **proc_data}\n",
    "nn_corr_df.loc[len(nn_corr_df)] = new_row\n",
    "\n",
    "model_history(history, f'Correlation Threshold = 0.85')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Threshold = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5-D1-x1')\n",
    "\n",
    "X_train_090 = XData_train.drop(columns=df_corr_comp.loc[0.90]['to_drop'])\n",
    "X_test_090 = XData_test.drop(columns=df_corr_comp.loc[0.90]['to_drop'])\n",
    "X_val_090 = XData_val.drop(columns=df_corr_comp.loc[0.90]['to_drop'])\n",
    "\n",
    "model.add(layers.Input((X_train_090.shape[1],)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "scaler_090 = RobustScaler()\n",
    "X_train_090 = scaler_090.fit_transform(X_train_090)\n",
    "X_test_090 = scaler_090.transform(X_test_090)\n",
    "X_val_090 = scaler_090.transform(X_val_090)\n",
    "\n",
    "dataset = {'X_train': X_train_090, 'y_train': y_train, 'X_val': X_val_090, 'y_val': y_val}\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0, Dataset=dataset)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Threshold': 0.90, **proc_data}\n",
    "nn_corr_df.loc[len(nn_corr_df)] = new_row\n",
    "\n",
    "model_history(history, f'Correlation Threshold = 0.90')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Threshold = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "model = models.Sequential(name=f'NN_30x30x5-D1-x1')\n",
    "\n",
    "X_train_095 = XData_train.drop(columns=df_corr_comp.loc[0.95]['to_drop'])\n",
    "X_test_095 = XData_test.drop(columns=df_corr_comp.loc[0.95]['to_drop'])\n",
    "X_val_095 = XData_val.drop(columns=df_corr_comp.loc[0.95]['to_drop'])\n",
    "\n",
    "model.add(layers.Input((X_train_095.shape[1],)))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "scaler_095 = RobustScaler()\n",
    "X_train_095 = scaler_095.fit_transform(X_train_095)\n",
    "X_test_095 = scaler_095.transform(X_test_095)\n",
    "X_val_095 = scaler_095.transform(X_val_095)\n",
    "\n",
    "dataset = {'X_train': X_train_095, 'y_train': y_train, 'X_val': X_val_095, 'y_val': y_val}\n",
    "history, model = model_fit(model, epochNum=1000, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0, Dataset=dataset)\n",
    "proc_data = proc_history(history)\n",
    "\n",
    "new_row = {'Threshold': 0.95, **proc_data}\n",
    "nn_corr_df.loc[len(nn_corr_df)] = new_row\n",
    "\n",
    "model_history(history, f'Correlation Threshold = 0.95')\n",
    "proc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: In our case, dropping features that are correlated didn't help the model to learn better. <br>\n",
    "We do see that even with fewer features, the model can achieve good enough results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
