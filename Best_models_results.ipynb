{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Models Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "notebook_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.regularizers import l1, l2\n",
    "#from keras.layers import Dropout, Flatten, BatchNormalization\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import callbacks\n",
    "from keras import utils\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset and split into Features and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df=pd.read_csv(\"breast-cancer-wisconsin-data/data.csv\")\n",
    "#drop irelevent columns for the classification\n",
    "df = df.drop(columns=['Unnamed: 32', 'id'])\n",
    "# rearange the data for X - featuers and Y leabels \n",
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mapping of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 0, 'M': 1}\n"
     ]
    }
   ],
   "source": [
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into Train, Test and Valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "XData_train_val, XData_test, ydata_train_val, ydata_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "XData_train, XData_val, ydata_train, ydata_val = train_test_split(XData_train_val, ydata_train_val, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(XData_train)\n",
    "X_test = scaler.transform(XData_test)\n",
    "X_val = scaler.transform(XData_val)\n",
    "\n",
    "X_train_val = XData_train_val.values\n",
    "\n",
    "y_train = ydata_train\n",
    "y_test = ydata_test\n",
    "y_val = ydata_val\n",
    "\n",
    "y_train_val = ydata_train_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling the imbalance between the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.774468085106383, 1: 1.4108527131782946}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#handling imbalance data \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "#the class weight is done only on the train data to impact the learning process and to evaluete beter the model proformence\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(model, X, y, dataset_name):\n",
    "    print('================================================================================')\n",
    "    print(f'Evaluation Report for Model: {model.name} on {dataset_name}')\n",
    "\n",
    "    # accuracy\n",
    "    result = model.evaluate(X, y, verbose=0)        \n",
    "    print(f'Loss Value: {result[0]:.3f}, Accuracy: {result[1]*100:.3f}%')\n",
    "\n",
    "    # confusion matrix\n",
    "    y_prob = model.predict(X, verbose=0)\n",
    "    y_pred = np.round(y_prob).astype(int)    \n",
    "    cm = confusion_matrix(y, y_pred)    \n",
    "    #sns.heatmap(cm,annot=True)\n",
    "    cm_disp = ConfusionMatrixDisplay(cm, display_labels=le.classes_)\n",
    "    cm_disp.plot()\n",
    "    plt.title(f'{model.name} Confusion Matrix on {dataset_name} Dataset')\n",
    "    plt.show()\n",
    "\n",
    "    # classification report\n",
    "    report = classification_report(y, y_pred, target_names=le.classes_)\n",
    "    print(report)\n",
    "\n",
    "    print('Model Summary:')\n",
    "    print(model.summary())\n",
    "    print('================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_history(history, param=None):\n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    val_loss_min_pos = np.argmin(history.history['val_loss'])\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', color='#8502d1')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "    if param is None:\n",
    "        plt.title('Train and Validation Loss')\n",
    "    else:\n",
    "        plt.title(f'Train and Validation Loss on {param}')\n",
    "\n",
    "    plt.plot(val_loss_min_pos, history.history['val_loss'][val_loss_min_pos], 'r*', label='Min Validation Loss')\n",
    "    plt.text(val_loss_min_pos, history.history['val_loss'][val_loss_min_pos], f'({val_loss_min_pos}, {history.history['val_loss'][val_loss_min_pos]:.3f})', va='bottom')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting the training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', color='#8502d1')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.plot(val_loss_min_pos, history.history['val_accuracy'][val_loss_min_pos], 'r*', label='Validation Accuracy @ Min Validation Loss')\n",
    "    plt.text(val_loss_min_pos, history.history['val_accuracy'][val_loss_min_pos], f'({val_loss_min_pos}, {history.history['val_accuracy'][val_loss_min_pos]:.3f})', va='bottom')\n",
    "\n",
    "    if param is None:\n",
    "        plt.title('Train and Validation Accuracy')\n",
    "    else:\n",
    "        plt.title(f'Train and Validation Accuracy on {param}')\n",
    "        \n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_history(history):\n",
    "\n",
    "    val_loss_min_pos = np.argmin(history.history['val_loss'])\n",
    "\n",
    "    train_acc = history.history['accuracy'][val_loss_min_pos]\n",
    "    train_loss = history.history['loss'][val_loss_min_pos]\n",
    "\n",
    "    val_acc = history.history['val_accuracy'][val_loss_min_pos]\n",
    "    val_loss = history.history['val_loss'][val_loss_min_pos]    \n",
    "\n",
    "    val_recall = history.history['val_recall'][val_loss_min_pos]\n",
    "\n",
    "    return {'Train Accuracy': train_acc, 'Train Loss': train_loss, 'Validation Accuracy': val_acc, 'Validation Loss': val_loss, 'Validation Recall': val_recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model, optimizer=optimizers.RMSprop, learning_rate=0.001, epochNum=1000, batchSize=32, en_reduce_lr=False, en_early_stopping=True, pca=False, verbose=\"auto\", Dataset=None):      \n",
    "    \n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001, verbose=0)\n",
    "    checkpoint_filepath = f'model_checkpoints/{model.name}_checkpoint.model.keras'\n",
    "    model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=0)\n",
    "    \n",
    "    callbacks_list = [model_checkpoint_callback]\n",
    "\n",
    "    if en_reduce_lr:\n",
    "        callbacks_list.append(reduce_lr)\n",
    "\n",
    "    if en_early_stopping:\n",
    "        callbacks_list.append(early_stopping)\n",
    "\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy', metrics.Recall(name='recall')])\n",
    "    if Dataset is None:\n",
    "        history = model.fit(X_train, y_train, batch_size=batchSize, epochs=epochNum, validation_data=(X_val, y_val), class_weight=class_weight_dict, callbacks=callbacks_list, verbose=verbose)\n",
    "    else:\n",
    "        history = model.fit(Dataset['X_train'], Dataset['y_train'], batch_size=batchSize, epochs=epochNum, validation_data=(Dataset['X_val'], Dataset['y_val']), class_weight=class_weight_dict, callbacks=callbacks_list, verbose=verbose)\n",
    "\n",
    "    model = models.load_model(checkpoint_filepath)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_kfold(xtrain, ytrain, model_base, optimizer=optimizers.RMSprop,learning_rate=0.001, epochNum=1000, batchSize=32, en_reduce_lr=False, en_early_stopping=True, verbose=\"auto\"):\n",
    "    model = models.clone_model(model_base)\n",
    "    fold_k = StratifiedKFold(n_splits = 5).split(xtrain, ytrain)\n",
    "    \n",
    "    result_df = pd.DataFrame(columns=['k', 'Train Accuracy', 'Train Loss' , 'Validation Accuracy', 'Validation Loss',  'Validation Recall'])\n",
    "    for k , (train, valid) in enumerate(fold_k):\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "\n",
    "        X_train = scaler.fit_transform(xtrain[train])\n",
    "        X_val = scaler.transform(xtrain[valid])\n",
    "        \n",
    "        dataset = {'X_train': X_train, 'y_train': ytrain[train], 'X_val': X_val, 'y_val': ytrain[valid]}\n",
    "        model.set_weights(model_base.get_weights())\n",
    "\n",
    "        \n",
    "\n",
    "        history, model = model_fit(model, optimizer=optimizer, learning_rate=learning_rate, epochNum=epochNum, batchSize=batchSize, en_reduce_lr=en_reduce_lr, en_early_stopping=en_early_stopping, Dataset=dataset, verbose=verbose)    \n",
    "        \n",
    "        proc_data = proc_history(history)\n",
    "\n",
    "        new_row = {'k': k, **proc_data}\n",
    "\n",
    "        result_df.loc[len(result_df)] = new_row\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODELS = {}\n",
    "BEST_MODELS_HISTORY = {}\n",
    "\n",
    "BEST_OPTIMIZER = optimizers.SGD\n",
    "BEST_LEARNING_RATE = 0.001\n",
    "BEST_BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "BEST_MODEL = models.Sequential(name=f'SLP')\n",
    "\n",
    "BEST_MODEL.add(layers.Input((30,)))\n",
    "BEST_MODEL.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "BEST_MODELS['SLP'] = BEST_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP 30x30x1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "BEST_MODEL = models.Sequential(name=f'MLP_30x30x1')\n",
    "\n",
    "BEST_MODEL.add(layers.Input((30,)))\n",
    "BEST_MODEL.add(layers.Dense(30, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "BEST_MODELS['MLP_30x30x1'] = BEST_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP 30x30x10x1 and at Second Layer: Dropout 0.1 & Activation Function Silu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "BEST_MODEL = models.Sequential(name=f'MLP_30x10x10-D1-silu-x1')\n",
    "\n",
    "BEST_MODEL.add(layers.Input((30,)))\n",
    "BEST_MODEL.add(layers.Dense(10, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dense(10, activation='silu'))\n",
    "BEST_MODEL.add(layers.Dropout(0.1))\n",
    "BEST_MODEL.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "BEST_MODELS['MLP_30x10x10-D1-silu-x1'] = BEST_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_best_models_df = pd.DataFrame(columns=['Model Name', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss', 'Validation Recall'])\n",
    "\n",
    "for model_name, model in BEST_MODELS.items():\n",
    "\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    print(f'Model Name = {model_name}')\n",
    "    \n",
    "    if model_name == 'SLP':\n",
    "        history, model = model_fit(model, epochNum=2500, batchSize=32, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "        res_df = model_kfold(X_train_val, y_train_val, model, epochNum=2500, batchSize=32, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "    elif model_name == 'MLP_30x30x1':\n",
    "        history, model = model_fit(model, epochNum=3500, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=32, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "        res_df = model_kfold(X_train_val, y_train_val, model, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, epochNum=3500, batchSize=32, en_early_stopping=True, en_reduce_lr=True, verbose=0)        \n",
    "    else:\n",
    "        history, model = model_fit(model, epochNum=2500, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "        res_df = model_kfold(X_train_val, y_train_val, model, epochNum=2500, batchSize=BEST_BATCH_SIZE, en_early_stopping=True, en_reduce_lr=True, verbose=0)\n",
    "\n",
    "    BEST_MODELS_HISTORY[model_name] = history\n",
    "    proc_data = proc_history(history)\n",
    "\n",
    "    new_row = {'Model Name': model.name, **proc_data}\n",
    "    nn_best_models_df.loc[len(nn_best_models_df)] = new_row\n",
    "\n",
    "    \n",
    "    print(f\"Validation Accuracy Mean: {res_df['Validation Loss'].mean()}, Validation Accuracy STD: {res_df['Validation Loss'].std()}\")    \n",
    "    print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "    print(res_df)\n",
    "\n",
    "    model_history(history, f'Model Name = {model.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_best_models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Accuracy Learning Curve of all all Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, history in BEST_MODELS_HISTORY.items():\n",
    "    plt.plot(history.history['val_accuracy'][:1000], label=model_name)\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy Learning Curve of all Best Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Loss Learning Curve of all all Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, history in BEST_MODELS_HISTORY.items():\n",
    "    plt.plot(history.history['val_loss'][:1000], label=model_name)\n",
    "plt.legend()\n",
    "plt.title('Validation Loss Learning Curve of all Best Models')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and Validation Accuracy of all Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = nn_best_models_df.plot.bar(x='Model Name', y=['Train Accuracy', 'Validation Accuracy'], rot=15, figsize=(10, 5))\n",
    "axes.legend(loc='lower right')\n",
    "\n",
    "for p in axes.patches:\n",
    "    axes.annotate(str(round(p.get_height(),3)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "\n",
    "_ = axes.set_title('Train and Validation Accuracy of Best Models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and Validation Loss of all Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = nn_best_models_df.plot.bar(x='Model Name', y=['Train Loss', 'Validation Loss'], rot=15, figsize=(10, 5))\n",
    "axes.legend(loc='lower right')\n",
    "\n",
    "for p in axes.patches:\n",
    "    axes.annotate(str(round(p.get_height(),3)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "\n",
    "_ = axes.set_title('Train and Validation Loss of Best Models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train+Validation Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Evaluation on Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in BEST_MODELS.items():\n",
    "    model_result(model, X_train, y_train, 'Train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid Evaluation on Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in BEST_MODELS.items():\n",
    "    model_result(model, X_val, y_val, 'Valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1_Score, Precision, Recall and ROC on Validation Dataset of Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "nn_best_models_scores_df = pd.DataFrame(columns=['Model Name', 'Validation F1_Score', 'Validation Precision', 'Validation Recall'])\n",
    "\n",
    "for model_name, model in BEST_MODELS.items():\n",
    "\n",
    "    y_prob = model.predict(X_val, verbose=0)\n",
    "    y_pred = np.round(y_prob).astype(int)    \n",
    "\n",
    "    f1_score_res = f1_score(y_val, y_pred, labels=le.classes_)\n",
    "    precision_score_res = precision_score(y_val, y_pred, labels=le.classes_)\n",
    "    recall_score_res = recall_score(y_val, y_pred, labels=le.classes_)\n",
    "\n",
    "    new_row = {'Model Name': model.name, 'Validation F1_Score': f1_score_res, 'Validation Precision': precision_score_res, 'Validation Recall': recall_score_res}\n",
    "    nn_best_models_scores_df.loc[len(nn_best_models_scores_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_best_models_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = nn_best_models_scores_df.plot.bar(x='Model Name', y=['Validation F1_Score', 'Validation Precision', 'Validation Recall'], rot=15, figsize=(10, 5))\n",
    "axes.legend(loc='lower right')\n",
    "\n",
    "for p in axes.patches:\n",
    "    axes.annotate(str(round(p.get_height(),3)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "\n",
    "_ = axes.set_title('Validation F1_Score, Precision and Recall of Best Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "axes = plt.axes()\n",
    "for model_name, model in BEST_MODELS.items():\n",
    "    y_prob = model.predict(X_val, verbose=0)    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_val, y_prob)\n",
    "    print(f\"Model Name = {model_name}\")\n",
    "    print(f'FPR: {fpr}')\n",
    "    print(f'TPR: {tpr}')\n",
    "    print(f'Thresholds: {thresholds}')\n",
    "    roc_disp = RocCurveDisplay(fpr=fpr, tpr=tpr, estimator_name=model.name)\n",
    "\n",
    "    roc_disp.plot(ax=axes) \n",
    "\n",
    "    y_pred = np.round(y_prob).astype(int)    \n",
    "    pos10 = (y_val==1) & (y_pred==0).flatten()\n",
    "    X_failure_10 = X_val[pos10][:2]\n",
    "    print(f'Positions of the false prediction in the validation dataset: {np.where(pos10)[0]}')\n",
    "    print(f'Probability of the false prediciton: {y_prob[pos10]}')    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "axes = plt.axes()\n",
    "for model_name, model in BEST_MODELS.items():\n",
    "    y_prob = model.predict(X_val, verbose=0)    \n",
    "    pos1 = (y_val==1)\n",
    "\n",
    "    plt.plot(y_prob[pos1], '.-', label=model.name)   \n",
    "\n",
    "plt.title('Prediction probability of True Malignant in Validation Dataset (Higher is better)')\n",
    "plt.ylabel('Prediction probability')\n",
    "plt.xlabel('Examples of True Malignant')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "axes = plt.axes()\n",
    "for model_name, model in BEST_MODELS.items():\n",
    "    y_prob = model.predict(X_val, verbose=0)    \n",
    "    pos0 = (y_val==0)\n",
    "\n",
    "    plt.plot(y_prob[pos0], '.-', label=model.name)   \n",
    "\n",
    "plt.title('Prediction probability of True Benign in Validation Dataset (Lower is better)')\n",
    "plt.ylabel('Prediction probability')\n",
    "plt.xlabel('Examples of True Benign')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test_best_models_df = nn_best_models_df.drop(columns=['Train Accuracy', 'Train Loss'])\n",
    "nn_test_best_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res_df = pd.DataFrame(columns=['Test Accuracy', 'Test Loss', 'Test Recall'])\n",
    "\n",
    "\n",
    "for model_name, model in BEST_MODELS.items():\n",
    "    res_dict = model.evaluate(X_test, y_test, verbose=0, return_dict=True)\n",
    "    res_df.loc[len(res_df)] = {'Test Accuracy': res_dict['accuracy'], 'Test Loss': res_dict['loss'], 'Test Recall': res_dict['recall']}    \n",
    "\n",
    "nn_test_best_models_df = pd.concat([nn_test_best_models_df, res_df], axis=1, join='inner')    \n",
    "nn_test_best_models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation and Test Accuracy of all Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = nn_test_best_models_df.plot.bar(x='Model Name', y=['Test Accuracy', 'Validation Accuracy'], rot=15, figsize=(10, 5), color=['r', 'orange'])\n",
    "axes.legend(loc='lower right')\n",
    "\n",
    "for p in axes.patches:\n",
    "    axes.annotate(str(round(p.get_height(),3)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "\n",
    "_ = axes.set_title('Test and Validation Accuracy of Best Models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and Validation Loss of all Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = nn_test_best_models_df.plot.bar(x='Model Name', y=['Test Loss', 'Validation Loss'], rot=15, figsize=(10, 5), color=['r', 'orange'])\n",
    "axes.legend(loc='lower right')\n",
    "\n",
    "for p in axes.patches:\n",
    "    axes.annotate(str(round(p.get_height(),3)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "\n",
    "_ = axes.set_title('Test and Validation Loss of Best Models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Evaluation on Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in BEST_MODELS.items():\n",
    "    model_result(model, X_test, y_test, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1_Score, Precision, Recall and ROC on Test Dataset of Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "nn_best_models_scores_df = pd.DataFrame(columns=['Model Name', 'Test F1_Score', 'Test Precision', 'Test Recall'])\n",
    "\n",
    "for model_name, model in BEST_MODELS.items():\n",
    "\n",
    "    y_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.round(y_prob).astype(int)    \n",
    "\n",
    "    f1_score_res = f1_score(y_test, y_pred, labels=le.classes_)\n",
    "    precision_score_res = precision_score(y_test, y_pred, labels=le.classes_)\n",
    "    recall_score_res = recall_score(y_test, y_pred, labels=le.classes_)\n",
    "\n",
    "    new_row = {'Model Name': model.name, 'Test F1_Score': f1_score_res, 'Test Precision': precision_score_res, 'Test Recall': recall_score_res}\n",
    "    nn_best_models_scores_df.loc[len(nn_best_models_scores_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_best_models_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = nn_best_models_scores_df.plot.bar(x='Model Name', y=['Test F1_Score', 'Test Precision', 'Test Recall'], rot=15, figsize=(10, 5))\n",
    "axes.legend(loc='lower right')\n",
    "\n",
    "for p in axes.patches:\n",
    "    axes.annotate(str(round(p.get_height(),3)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "\n",
    "_ = axes.set_title('Test F1_Score, Precision and Recall on Test Dataset of Best Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "axes = plt.axes()\n",
    "for model_name, model in BEST_MODELS.items():\n",
    "    y_prob = model.predict(X_test, verbose=0)    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob)\n",
    "\n",
    "    print(f'FPR: {fpr}')\n",
    "    print(f'TPR: {tpr}')\n",
    "    print(f'Thresholds: {thresholds}')\n",
    "    roc_disp = RocCurveDisplay(fpr=fpr, tpr=tpr, estimator_name=model.name)\n",
    "\n",
    "    roc_disp.plot(ax=axes) \n",
    "\n",
    "    y_pred = np.round(y_prob).astype(int)    \n",
    "    pos10 = (y_test==1) & (y_pred==0).flatten()\n",
    "    X_failure_10 = X_test[pos10][:2]\n",
    "    print(f'Positions of the false prediction in the test dataset: {np.where(pos10)[0]}')\n",
    "    print(f'Probability of the false prediciton: {y_prob[pos10]}')    \n",
    "\n",
    "plt.title('ROC on Test Dataset of Best Models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "axes = plt.axes()\n",
    "for model_name, model in BEST_MODELS.items():\n",
    "    y_prob = model.predict(X_test, verbose=0)    \n",
    "    pos1 = (y_test==1)\n",
    "\n",
    "    plt.plot(y_prob[pos1], '.-', label=model.name)   \n",
    "\n",
    "plt.title('Prediction probability of True Malignant in Test Dataset (Higher is better)')\n",
    "plt.ylabel('Prediction probability')\n",
    "plt.xlabel('Examples of True Malignant')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "axes = plt.axes()\n",
    "for model_name, model in BEST_MODELS.items():\n",
    "    y_prob = model.predict(X_test, verbose=0)    \n",
    "    pos0 = (y_test==0)\n",
    "\n",
    "    plt.plot(y_prob[pos0], '.-', label=model.name)   \n",
    "\n",
    "plt.title('Prediction probability of True Benign in Test Dataset (Lower is better)')\n",
    "plt.ylabel('Prediction probability')\n",
    "plt.xlabel('Examples of True Benign')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cases of Success and Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BEST_MODELS['SLP']\n",
    "\n",
    "y_prob = model.predict(X_test, verbose=0)\n",
    "y_pred = np.round(y_prob).astype(int) \n",
    "y_pred = y_pred.flatten() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truth Benign, Predicted Bengin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos00 = (y_test==0) & (y_pred==0).flatten()\n",
    "X_success_00 = X_test[pos00][[0]]\n",
    "pd.DataFrame(X_success_00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truth Malignant, Predicted Malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos11 = (y_test==1) & (y_pred==1).flatten()\n",
    "X_success_11 = X_test[pos11][:2]\n",
    "pd.DataFrame(X_success_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model Failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truth Benign, Predicted Malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test==0) & (y_pred==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos01 = (y_test==0) & (y_pred==1).flatten()\n",
    "X_failure_01 = X_test[pos01][:]\n",
    "pd.DataFrame(X_failure_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truth Malignant, Predicted Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos10 = (y_test==1) & (y_pred==0).flatten()\n",
    "X_failure_10 = X_test[pos10][:2]\n",
    "pd.DataFrame(X_failure_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Successful Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1 - (TN) - Benign classified as Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_num = np.array([1])\n",
    "\n",
    "data_B_mean = X[y==0].mean()\n",
    "data_B_std = X[y==0].std()\n",
    "\n",
    "wrong_classified_data = scaler.inverse_transform(X_test)[example_num].T.flatten()\n",
    "wrong_classified_data = pd.Series(wrong_classified_data, index=data_B_mean.index)\n",
    "\n",
    "wrong_classified_data_n_2 = (wrong_classified_data-data_B_mean)/data_B_std\n",
    "\n",
    "res_df = pd.concat([wrong_classified_data, data_B_mean, data_B_std, wrong_classified_data_n_2], axis=1)\n",
    "res_df = res_df.rename(columns={0: 'Example', 1: 'Feature Mean', 2: 'Feature STD', 3: 'Normalized Example'})\n",
    "res_df.style.apply(lambda x: [\"background: green\" if idx==3 and abs(v)>abs(3*x.iloc[2]) else \"\" for idx,v in enumerate(x)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 106 - (TP) - Malignant classified as Malignant on all the Best Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_num = np.array([106])\n",
    "\n",
    "data_M_mean = X[y==1].mean()\n",
    "data_M_std = X[y==1].std()\n",
    "\n",
    "wrong_classified_data = scaler.inverse_transform(X_test)[example_num].T.flatten()\n",
    "wrong_classified_data = pd.Series(wrong_classified_data, index=data_M_mean.index)\n",
    "\n",
    "wrong_classified_data_n_2 = (wrong_classified_data-data_M_mean)/data_M_std\n",
    "\n",
    "res_df = pd.concat([wrong_classified_data, data_M_mean, data_M_std, wrong_classified_data_n_2], axis=1)\n",
    "res_df = res_df.rename(columns={0: 'Example', 1: 'Feature Mean', 2: 'Feature STD', 3: 'Normalized Example'})\n",
    "res_df.style.apply(lambda x: [\"background: green\" if idx==3 and abs(v)>abs(3*x.iloc[2]) else \"\" for idx,v in enumerate(x)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 112 - (TP) - Malignant classified as Malignant on all the Best Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_num = np.array([112])\n",
    "\n",
    "data_M_mean = X[y==1].mean()\n",
    "data_M_std = X[y==1].std()\n",
    "\n",
    "wrong_classified_data = scaler.inverse_transform(X_test)[example_num].T.flatten()\n",
    "wrong_classified_data = pd.Series(wrong_classified_data, index=data_M_mean.index)\n",
    "\n",
    "wrong_classified_data_n_2 = (wrong_classified_data-data_M_mean)/data_M_std\n",
    "\n",
    "res_df = pd.concat([wrong_classified_data, data_M_mean, data_M_std, wrong_classified_data_n_2], axis=1)\n",
    "res_df = res_df.rename(columns={0: 'Example', 1: 'Feature Mean', 2: 'Feature STD', 3: 'Normalized Example'})\n",
    "res_df.style.apply(lambda x: [\"background: green\" if idx==3 and abs(v)>abs(3*x.iloc[2]) else \"\" for idx,v in enumerate(x)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Failure Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 108 - (FN) - Malignant classified as Benign on most of the Best Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_num = np.array([108])\n",
    "\n",
    "data_M_mean = X[y==1].mean()\n",
    "data_M_std = X[y==1].std()\n",
    "\n",
    "wrong_classified_data = scaler.inverse_transform(X_test)[example_num].T.flatten()\n",
    "wrong_classified_data = pd.Series(wrong_classified_data, index=data_M_mean.index)\n",
    "\n",
    "wrong_classified_data_n_2 = (wrong_classified_data-data_M_mean)/data_M_std\n",
    "\n",
    "res_df = pd.concat([wrong_classified_data, data_M_mean, data_M_std, wrong_classified_data_n_2], axis=1)\n",
    "res_df = res_df.rename(columns={0: 'Example', 1: 'Feature Mean', 2: 'Feature STD', 3: 'Normalized Example'})\n",
    "res_df.style.apply(lambda x: [\"background: green\" if idx==3 and abs(v)>abs(3*x.iloc[2]) else \"\" for idx,v in enumerate(x)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 109 - (FN) - Malignant classified as Benign on all the Best Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_num = np.array([109])\n",
    "\n",
    "data_M_mean = X[y==1].mean()\n",
    "data_M_std = X[y==1].std()\n",
    "\n",
    "wrong_classified_data = scaler.inverse_transform(X_test)[example_num].T.flatten()\n",
    "wrong_classified_data = pd.Series(wrong_classified_data, index=data_M_mean.index)\n",
    "\n",
    "wrong_classified_data_n_2 = (wrong_classified_data-data_M_mean)/data_M_std\n",
    "\n",
    "res_df = pd.concat([wrong_classified_data, data_M_mean, data_M_std, wrong_classified_data_n_2], axis=1)\n",
    "res_df = res_df.rename(columns={0: 'Example', 1: 'Feature Mean', 2: 'Feature STD', 3: 'Normalized Example'})\n",
    "res_df.style.apply(lambda x: [\"background: green\" if idx==3 and abs(v)>abs(3*x.iloc[2]) else \"\" for idx,v in enumerate(x)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 13 - (FP) - Benign classified as Malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_num = np.array([13])\n",
    "\n",
    "data_B_mean = X[y==0].mean()\n",
    "data_B_std = X[y==0].std()\n",
    "\n",
    "wrong_classified_data = scaler.inverse_transform(X_test)[example_num].T.flatten()\n",
    "wrong_classified_data = pd.Series(wrong_classified_data, index=data_B_mean.index)\n",
    "\n",
    "wrong_classified_data_n_2 = (wrong_classified_data-data_B_mean)/data_B_std\n",
    "\n",
    "res_df = pd.concat([wrong_classified_data, data_B_mean, data_B_std, wrong_classified_data_n_2], axis=1)\n",
    "res_df = res_df.rename(columns={0: 'Example', 1: 'Feature Mean', 2: 'Feature STD', 3: 'Normalized Example'})\n",
    "res_df.style.apply(lambda x: [\"background: green\" if idx==3 and abs(v)>abs(3*x.iloc[2]) else \"\" for idx,v in enumerate(x)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Models Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODELS_BASE = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic fitting   \n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "BEST_MODEL = models.Sequential(name=f'SLP')\n",
    "\n",
    "BEST_MODEL.add(layers.Input((30,)))\n",
    "BEST_MODEL.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "BEST_MODELS_BASE['SLP'] = BEST_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP 30x30x1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "BEST_MODEL = models.Sequential(name=f'MLP_30x30x1')\n",
    "\n",
    "BEST_MODEL.add(layers.Input((30,)))\n",
    "BEST_MODEL.add(layers.Dense(30, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "BEST_MODELS_BASE['MLP_30x30x1'] = BEST_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP 30x30x10x1 and at Second Layer: Dropout 0.1 & Activation Function Silu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "\n",
    "BEST_MODEL = models.Sequential(name=f'MLP_30x10x10-D1-silu-x1')\n",
    "\n",
    "BEST_MODEL.add(layers.Input((30,)))\n",
    "BEST_MODEL.add(layers.Dense(10, activation='relu'))\n",
    "BEST_MODEL.add(layers.Dense(10, activation='silu'))\n",
    "BEST_MODEL.add(layers.Dropout(0.1))\n",
    "BEST_MODEL.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "BEST_MODELS_BASE['MLP_30x10x10-D1-silu-x1'] = BEST_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "utils.set_random_seed(0)\n",
    "tf.config.experimental.enable_op_determinism()  \n",
    "    \n",
    "result_df = {}\n",
    "for model_name, best_model in BEST_MODELS_BASE.items():\n",
    "\n",
    "    fold_k = StratifiedKFold(n_splits = 5).split(X.values, y)\n",
    "    \n",
    "    result_df[model_name] = pd.DataFrame(columns=['k', 'Train Accuracy', 'Train Loss' , 'Validation Accuracy', 'Validation Loss',  'Validation Recall'])\n",
    "\n",
    "    for k , (train, valid) in enumerate(fold_k):\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "\n",
    "        X_train_f = scaler.fit_transform(X.values[train])\n",
    "        X_val_f = scaler.transform(X.values[valid])\n",
    "        \n",
    "        dataset = {'X_train': X_train_f, 'y_train': y[train], 'X_val': X_val_f, 'y_val': y[valid]}\n",
    "        model = models.clone_model(best_model)\n",
    "        model.set_weights(BEST_MODELS_BASE[model_name].get_weights())\n",
    "\n",
    "        history, model = model_fit(model, optimizer=BEST_OPTIMIZER, learning_rate=BEST_LEARNING_RATE, epochNum=2500, batchSize=BEST_BATCH_SIZE, en_reduce_lr=True, en_early_stopping=True, Dataset=dataset, verbose=0)    \n",
    "        \n",
    "        proc_data = proc_history(history)\n",
    "\n",
    "        new_row = {'k': k, **proc_data}\n",
    "\n",
    "        result_df[model_name].loc[len(result_df[model_name])] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SLP':    k  Train Accuracy  Train Loss  Validation Accuracy  Validation Loss  \\\n",
       " 0  0        0.978022    0.065383             0.982456         0.064810   \n",
       " 1  1        0.984615    0.061824             0.956140         0.086625   \n",
       " 2  2        0.978022    0.078468             0.973684         0.070427   \n",
       " 3  3        0.982418    0.053811             0.973684         0.106260   \n",
       " 4  4        0.978070    0.061502             0.982301         0.068523   \n",
       " \n",
       "    Validation Recall  \n",
       " 0           0.976744  \n",
       " 1           0.953488  \n",
       " 2           0.976190  \n",
       " 3           0.952381  \n",
       " 4           1.000000  ,\n",
       " 'MLP_30x30x1':    k  Train Accuracy  Train Loss  Validation Accuracy  Validation Loss  \\\n",
       " 0  0        0.982418    0.035360             0.991228         0.055420   \n",
       " 1  1        0.986813    0.036418             0.964912         0.081204   \n",
       " 2  2        0.978022    0.088186             0.964912         0.096443   \n",
       " 3  3        0.986813    0.038147             0.973684         0.106251   \n",
       " 4  4        0.997807    0.018307             0.982301         0.052092   \n",
       " \n",
       "    Validation Recall  \n",
       " 0           0.976744  \n",
       " 1           0.930233  \n",
       " 2           0.976190  \n",
       " 3           0.952381  \n",
       " 4           1.000000  ,\n",
       " 'MLP_30x10x10-D1-silu-x1':    k  Train Accuracy  Train Loss  Validation Accuracy  Validation Loss  \\\n",
       " 0  0        0.982418    0.051955             0.956140         0.124006   \n",
       " 1  1        0.975824    0.070166             0.947368         0.106543   \n",
       " 2  2        0.971429    0.088107             0.973684         0.065959   \n",
       " 3  3        0.980220    0.050913             0.964912         0.091397   \n",
       " 4  4        0.982456    0.043729             0.982301         0.063967   \n",
       " \n",
       "    Validation Recall  \n",
       " 0           0.976744  \n",
       " 1           0.883721  \n",
       " 2           0.976190  \n",
       " 3           0.976190  \n",
       " 4           1.000000  }"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Results on Best Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLP K-Fold:\n",
      "Train Accuracy Mean: 0.9802294254302979, Train Accuracy STD: 0.003099700583179626\n",
      "Validation Accuracy Mean: 0.9736531496047973, Validation Accuracy STD: 0.010711867522905585\n",
      "Validation Loss Mean: 0.0793291449546814, Validation Loss STD: 0.017214117209590714\n",
      "Validation Recall Mean: 0.9717607855796814, Validation Recall STD: 0.01969371116721688\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.065383</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.064810</td>\n",
       "      <td>0.976744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.061824</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.086625</td>\n",
       "      <td>0.953488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.078468</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.070427</td>\n",
       "      <td>0.976190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.982418</td>\n",
       "      <td>0.053811</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.106260</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.978070</td>\n",
       "      <td>0.061502</td>\n",
       "      <td>0.982301</td>\n",
       "      <td>0.068523</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  Train Accuracy  Train Loss  Validation Accuracy  Validation Loss  \\\n",
       "0  0        0.978022    0.065383             0.982456         0.064810   \n",
       "1  1        0.984615    0.061824             0.956140         0.086625   \n",
       "2  2        0.978022    0.078468             0.973684         0.070427   \n",
       "3  3        0.982418    0.053811             0.973684         0.106260   \n",
       "4  4        0.978070    0.061502             0.982301         0.068523   \n",
       "\n",
       "   Validation Recall  \n",
       "0           0.976744  \n",
       "1           0.953488  \n",
       "2           0.976190  \n",
       "3           0.952381  \n",
       "4           1.000000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('SLP K-Fold:')\n",
    "res_df = result_df['SLP']\n",
    "print(f\"Train Accuracy Mean: {res_df['Train Accuracy'].mean()}, Train Accuracy STD: {res_df['Train Accuracy'].std()}\") \n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Accuracy'].mean()}, Validation Accuracy STD: {res_df['Validation Accuracy'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Recall Mean: {res_df['Validation Recall'].mean()}, Validation Recall STD: {res_df['Validation Recall'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP_30x30x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_30x30x1 K-Fold:\n",
      "Train Accuracy Mean: 0.9863745927810669, Train Accuracy STD: 0.007357128043880629\n",
      "Validation Accuracy Mean: 0.9754075407981873, Validation Accuracy STD: 0.011413552262145516\n",
      "Validation Loss Mean: 0.07828205525875091, Validation Loss STD: 0.024131153422157544\n",
      "Validation Recall Mean: 0.9671096324920654, Validation Recall STD: 0.026617252368817138\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.982418</td>\n",
       "      <td>0.035360</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.055420</td>\n",
       "      <td>0.976744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.036418</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.081204</td>\n",
       "      <td>0.930233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.088186</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.096443</td>\n",
       "      <td>0.976190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.038147</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.106251</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.997807</td>\n",
       "      <td>0.018307</td>\n",
       "      <td>0.982301</td>\n",
       "      <td>0.052092</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  Train Accuracy  Train Loss  Validation Accuracy  Validation Loss  \\\n",
       "0  0        0.982418    0.035360             0.991228         0.055420   \n",
       "1  1        0.986813    0.036418             0.964912         0.081204   \n",
       "2  2        0.978022    0.088186             0.964912         0.096443   \n",
       "3  3        0.986813    0.038147             0.973684         0.106251   \n",
       "4  4        0.997807    0.018307             0.982301         0.052092   \n",
       "\n",
       "   Validation Recall  \n",
       "0           0.976744  \n",
       "1           0.930233  \n",
       "2           0.976190  \n",
       "3           0.952381  \n",
       "4           1.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('MLP_30x30x1 K-Fold:')\n",
    "res_df = result_df['MLP_30x30x1']\n",
    "print(f\"Train Accuracy Mean: {res_df['Train Accuracy'].mean()}, Train Accuracy STD: {res_df['Train Accuracy'].std()}\") \n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Accuracy'].mean()}, Validation Accuracy STD: {res_df['Validation Accuracy'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Recall Mean: {res_df['Validation Recall'].mean()}, Validation Recall STD: {res_df['Validation Recall'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP_30x10x10-D1-silu-x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_30x10x10-D1-silu-x1 K-Fold:\n",
      "Train Accuracy Mean: 0.9784692525863647, Train Accuracy STD: 0.004772750541060379\n",
      "Validation Accuracy Mean: 0.9648812294006348, Validation Accuracy STD: 0.013820619544004176\n",
      "Validation Loss Mean: 0.09037462025880813, Validation Loss STD: 0.025918232302843774\n",
      "Validation Recall Mean: 0.9625692009925843, Validation Recall STD: 0.04524964219991074\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.982418</td>\n",
       "      <td>0.051955</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.124006</td>\n",
       "      <td>0.976744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.975824</td>\n",
       "      <td>0.070166</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.106543</td>\n",
       "      <td>0.883721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.088107</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.065959</td>\n",
       "      <td>0.976190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.980220</td>\n",
       "      <td>0.050913</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.091397</td>\n",
       "      <td>0.976190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.043729</td>\n",
       "      <td>0.982301</td>\n",
       "      <td>0.063967</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  Train Accuracy  Train Loss  Validation Accuracy  Validation Loss  \\\n",
       "0  0        0.982418    0.051955             0.956140         0.124006   \n",
       "1  1        0.975824    0.070166             0.947368         0.106543   \n",
       "2  2        0.971429    0.088107             0.973684         0.065959   \n",
       "3  3        0.980220    0.050913             0.964912         0.091397   \n",
       "4  4        0.982456    0.043729             0.982301         0.063967   \n",
       "\n",
       "   Validation Recall  \n",
       "0           0.976744  \n",
       "1           0.883721  \n",
       "2           0.976190  \n",
       "3           0.976190  \n",
       "4           1.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('MLP_30x10x10-D1-silu-x1 K-Fold:')\n",
    "res_df = result_df['MLP_30x10x10-D1-silu-x1']\n",
    "print(f\"Train Accuracy Mean: {res_df['Train Accuracy'].mean()}, Train Accuracy STD: {res_df['Train Accuracy'].std()}\") \n",
    "print(f\"Validation Accuracy Mean: {res_df['Validation Accuracy'].mean()}, Validation Accuracy STD: {res_df['Validation Accuracy'].std()}\")    \n",
    "print(f\"Validation Loss Mean: {res_df['Validation Loss'].mean()}, Validation Loss STD: {res_df['Validation Loss'].std()}\")    \n",
    "print(f\"Validation Recall Mean: {res_df['Validation Recall'].mean()}, Validation Recall STD: {res_df['Validation Recall'].std()}\")    \n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_end = time.time()\n",
    "notebook_elapsed = notebook_end-notebook_start\n",
    "print('Finished Notebook Run!')\n",
    "print(f'Elapsed Run Time: {math.floor(notebook_elapsed/3600)} (h), {math.floor(notebook_elapsed%3600/60)} (m), {math.floor(notebook_elapsed%60)} (s)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
