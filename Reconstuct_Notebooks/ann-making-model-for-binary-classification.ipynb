{"cells":[{"metadata":{"_cell_guid":"4498ebbb-beac-4a85-a2dc-533d664588ce","_uuid":"017b79012bfea8cdeecd008194da893189bdc264"},"cell_type":"markdown","source":"# 1. Intro\nHi, Kagglers!\nI'm back with Python version.\nThanks to your attention, I've got 3rd place on this dataset with R!\n* **Classification(R ver.) [ Breast Cancer or Not (with 15 ML)](https://www.kaggle.com/mirichoi0218/classification-breast-cancer-or-not-with-15-ml).**\n\nFor ANN Beginners like me, I hope this kernel helps you to understand Neural Network Algorithms, especially SLP(Single-Layer Perceptron).\nAt the end of this kernel, I hope you can make your own ANN model!"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"---\n\n# 2. What is ANN & SLP?\n### 2-1) ANN(Artificial Neural Network)\n* The Artificial Neural Network consists of an input layer, a hidden layer, and an output layer.\n\n![Imgur](https://elogeel.files.wordpress.com/2010/05/050510_1627_multilayerp1.png)\n\n\n### 2-2) SLP(Single Layer Perceptron)\n\n* If ANN model has no hidden layer, it is called single layer perceptron. \n\n![Imgur](https://www.analyticsvidhya.com/wp-content/uploads/2016/07/SLP.png)\n\n\n### 2-3) Basic equation of ANN-SLP\n* **Output = Weight * Input + Bias**\n* For this equation, we already have output, input layers. But don't have weight value and bias value.\n* Weight : a value that can give different weights depending on features and output \n    => [len(features), len(output)]\n* bias : a value that can give different weights depending on features\n    => [len(output)]\n\n\n### 2-4) MLP(Multi Layer Perceptron)\n* MLP(Multiple Layer Perceptron) model is ANN which has multiple hidden layers (more than 1) \n\n![Imgur](https://www.cc.gatech.edu/~san37/img/dl/mlp.png)\n\n\n### 2-5) Basic equation of ANN-SLP\n* **Output = (Weight1 x Input1) + (Weight2 x Input2) + ... + (WeightN x InputN) + Bias**\n* For this equation, we already have output, input layers. But don't have weight value and bias value.\n* Weight : a value that can give different weights depending on features and output \n    => [len(features), len(output)]\n* bias : a value that can give different weights depending on features\n    => [len(output)]\n\n\n---\n\n# 3. Import Libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer\nfrom sklearn.decomposition import PCA as sklearnPCA\n\n# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"62d537aa-38c0-4eb7-a8e3-b1306b8bf741","_uuid":"9388f2b05168c6dde3c3ae8c947a3ffcbf1acf9a"},"cell_type":"markdown","source":"---\n\n# 4. Explore Dataset\n## 4-1) Import dataset"},{"metadata":{"_cell_guid":"3f44bd61-891a-4907-9ad4-4b24e2a476de","collapsed":true,"_uuid":"226c6831972f96b5d8b50b66a6ff517c175b3e3b","trusted":false},"cell_type":"code","source":"wbcd = pd.read_csv(\"../input/data.csv\")\nwbcd.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4478f2f7-c82d-4b1d-8cc3-3a3d300c66f1","collapsed":true,"_uuid":"5904e176a7d70ca730bafc6eacbe2d8e4ab693ac","trusted":false},"cell_type":"code","source":"print(\"This WBCD dataset is consisted of\",wbcd.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9536eacc-1a47-4633-b2e0-7d59860f42c7","_uuid":"470e19dc549f573e737499d6a01be35d630c41b1"},"cell_type":"markdown","source":"## 4-2) Eliminate NaN value column"},{"metadata":{"_cell_guid":"5cfd1f39-8e74-4151-a117-d989fb2c8eaa","collapsed":true,"_uuid":"7139966be0eae747e222b64fe0080d893c7a62a4","trusted":false},"cell_type":"code","source":"wbcd = wbcd.iloc[:,:-1]\nprint(\"This WBCD dataset is consisted of\",wbcd.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa40b614-5d70-492e-a672-de808a12dcde","_uuid":"4b3285686b3d66efcf5d5441be5ebf4eeb695c3b"},"cell_type":"markdown","source":"## 4-3) Summary the Diagnosis"},{"metadata":{"_cell_guid":"52ad3062-d000-49c5-94a7-4ad80c3a25de","collapsed":true,"_uuid":"a8d564395cfb9b4b6c2758d37546a2d4074906f5","trusted":false},"cell_type":"code","source":"sns.countplot(wbcd['diagnosis'],label=\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5460d21-3fc8-4e6d-ba23-acf4b0fdb354","_uuid":"99491c255fb20c56cf862fb91fbe39830b9cff49"},"cell_type":"markdown","source":"## 4-4) Correlation Plot of 30 features\nexcept **id, diagnosis** columns => wbcd.iloc[:,2:]"},{"metadata":{"_cell_guid":"1d89200b-ab00-4a5d-b94b-9fac3ac11fc6","collapsed":true,"_uuid":"5e640fffd0e3e9ade799a75d3ac417d4a894916e","trusted":false},"cell_type":"code","source":"corr = wbcd.iloc[:,2:].corr()\ncolormap = sns.diverging_palette(220, 10, as_cmap = True)\nplt.figure(figsize=(14,14))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},\n            cmap = colormap, linewidths=0.1, linecolor='white')\nplt.title('Correlation of WBCD Features', y=1.05, size=15)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fb7abe15-91ec-455c-9f06-488310ae10ef","_uuid":"fd8c5caefa7ef7df8c9cfbe7c2fe5cfc047e1290"},"cell_type":"markdown","source":"---\n\n# 5. Preparing Data for machine learning\n## 5-1) Divide \"WBCD data\" into Train(70%) / Test data(30%)\nDivide the data into two(train/test) to see the predictive power of the model.\n"},{"metadata":{"_cell_guid":"010c6214-6da0-4a5c-9d0f-f5bf6a992790","collapsed":true,"_uuid":"c7aa69db4e553eab98f8be59cbf1731746164b94","trusted":false},"cell_type":"code","source":"train,test = train_test_split(wbcd, test_size=0.3, random_state=42)\nprint(\"Training Data :\",train.shape)\nprint(\"Testing Data :\",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3afd3829-911e-4ab3-bbee-a058580a7345","_uuid":"46e2739a863816cf2e55cded57836bf50d6385a8"},"cell_type":"markdown","source":"## 5-2) Drop ID column\n* Save the **ID** column for later combination(results).\n* Drop the **ID** column in train, test datasets, because it's unnecessary for model learning predictions."},{"metadata":{"_cell_guid":"6c4466ae-c775-4eaa-99e2-ae81a6f55f89","collapsed":true,"_uuid":"d6d0f79538cf52db3322f311242d85447d7e0646","trusted":false},"cell_type":"code","source":"train_id = train['id']\ntest_id = test['id']\n\ntrain_data = train.iloc[:,1:]\ntest_data = test.iloc[:,1:]\n\nprint(\"Training Data :\",train_data.shape)\nprint(\"Testing Data :\",test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bfeffbae-bf0b-4ed9-880d-28b840bc57fb","_uuid":"bb47e8bb957d9eb07332f7f58e9e3dbf52e8d1dd"},"cell_type":"markdown","source":"## 5-3) Seperate x:Feature data(30) / y:Result data(1)\nSeperate by **x_data, y_data**\n* x_data : columns(features to predict **diagnosis**) for training. (eliminate diagnosis)\n* y_data : columns for comparing with predictions results. (need original diagnosis)\n\n### 5-3-1) Normalize x_data values for better prediction"},{"metadata":{"_cell_guid":"62fd770f-f0ed-41da-95fe-2f96c4014024","collapsed":true,"_uuid":"e697e0b948b2549d9320dd5af6220e6fd57b8b26","trusted":false},"cell_type":"code","source":"# Training Data\ntrain_x = train_data.iloc[:,1:]\ntrain_x = MinMaxScaler().fit_transform(train_x)\nprint(\"Training Data :\", train_x.shape)\n\n# Testing Data\ntest_x = test_data.iloc[:,1:]\ntest_x = MinMaxScaler().fit_transform(test_x)\nprint(\"Testing Data :\", test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fd11f718-18bb-4ff2-9892-bbf3ee221945","_uuid":"cf139095fcb3ed9a2073ebbb2da5be2fadcf562d"},"cell_type":"markdown","source":"### 5-3-2) Change Results(diagnosis) format : String -> Numeric"},{"metadata":{"_cell_guid":"bb419590-3d9c-47ff-9df2-56ccdd7eacf7","collapsed":true,"_uuid":"eda3af7c8969e6b2853f2b6380d48a51aa5a7b05","trusted":false},"cell_type":"code","source":"# Training Data\ntrain_y = train_data.iloc[:,:1]\ntrain_y[train_y=='M'] = 0\ntrain_y[train_y=='B'] = 1\nprint(\"Training Data :\", train_y.shape)\n\n# Testing Data\ntest_y = test_data.iloc[:,:1]\ntest_y[test_y=='M'] = 0\ntest_y[test_y=='B'] = 1\nprint(\"Testing Data :\", test_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fa3ff52a-9d01-46af-ba41-945d6f960f3b","_uuid":"997e336dc4501dc0dcb5db2c1f7fcac363b381f3"},"cell_type":"markdown","source":"---\n\n# 6. Make ANN-SLP Model\n## 6-1) Make \"Placeholder\" for dinamic variable allocation\nPlaceholder is one of the function in tensorflow.\nIt is a space to put and change values while the program is running.\n* for X, a place must have 30 columns, since wbcd data has 30 features.\n* for Y, a place must have 1 columns, since the results has 1 outcome.\n* If you see the row \"None\", it means it has no size limits. (You can write -1 instead of \"None\")"},{"metadata":{"_cell_guid":"b2f628ba-b54f-4606-88b3-a94e8d04abfc","collapsed":true,"_uuid":"203de4b9aea972e31c4887e81b69011e978b4ae2","trusted":false},"cell_type":"code","source":"X = tf.placeholder(tf.float32, [None,30])\nY = tf.placeholder(tf.float32, [None, 1])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8b4db30e-6d9a-4218-a13c-77646f448e06","_uuid":"6150a33f862340ace07c778141028d9e7ef43c9a"},"cell_type":"markdown","source":"## 6-2) Make Weight, Bias value with randomly\n* W(weight) : why **[30,1]**?  16 for 16 features, 1 for 1 Outcome(results).\n* P(weight): why **[10,1]**? 10 for 10 PCA features, 1 for 1 Outcome(results).\n* b(bias) : why **[1]**?  outcome has 1 layers."},{"metadata":{"_cell_guid":"d1f84757-a576-4e5b-8906-bce6dba3c84c","collapsed":true,"_uuid":"e8791b6931c8197198f85a99f20e777697774906","trusted":false},"cell_type":"code","source":"# weight\nW = tf.Variable(tf.random_normal([30,1], seed=0), name='weight')\n\n# bias\nb = tf.Variable(tf.random_normal([1], seed=0), name='bias')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"97f607ce-213d-4e4a-a8e9-cc30c47d68a1","_uuid":"a16646d548cc2bf15303c815a12627e326fb7d7c"},"cell_type":"markdown","source":"## 6-3) Make Output Results\n * **Output = Weight * Input + Bias**\n * tf.matmul() : for array multiply"},{"metadata":{"_cell_guid":"56416585-56a4-442d-88f8-6cb2fb9f9101","collapsed":true,"_uuid":"0fe1de012691c4da71e4b5dc9324204bd61cebae","trusted":false},"cell_type":"code","source":"logits = tf.matmul(X,W) + b","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6d85b3f5-d900-4889-92f3-23cb539f6ed9","_uuid":"eb674379b6d9312560f7b8bbc9d87da457f3d181"},"cell_type":"markdown","source":"## 6-4) Cross Entropy\nBefore this, you have to know **How Linear Regression Works**\n* Linear Regression: Draw a random line to find the **mean square root error** and find the slope and intercept to minimize this value (reduce the error to the minimum)\n* Since Logits is also linear equation, you have to find minimum cost!\n\n![Imgur](https://machinelearningblogcom.files.wordpress.com/2018/01/bildschirmfoto-2018-01-24-um-14-32-02.png?w=1400)\n\nFor example, logits(we get above) is **red line**, and the real dataset is **blue dot**. \n1. For finding cost, you have to substract all blue dot value with red line. \n2. Next, You add all distance you find and get average. \n3. For good prediction, this average distance of red line & blue dot must be minimum value. \n\n* tf.nn.sigmoid_cross_entropy_with_logits(): for gradient_descent with sig results(hypothesis)."},{"metadata":{"_cell_guid":"4881aae2-320c-4af2-a243-41c11a8f0c34","collapsed":true,"_uuid":"9f792f4ee5c1fdc31de05ca8d4bb68c600bba200","trusted":false},"cell_type":"code","source":"hypothesis = tf.nn.sigmoid(logits)\n\ncost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\ncost = tf.reduce_mean(cost_i)\n# cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"980d8382-b7b7-44ad-8b0e-cd13b2d2f149","_uuid":"c2965cd1421afd8e83fc8596a34298e7be5c7e15"},"cell_type":"markdown","source":"## 6-5) Gradient Descent Optimizer\n\n![Imgur](http://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_gradient_descent_1.png)\n\n* GradientDescentOptimizer: It makes the best result with the least error\n* There are lots of optimizer methods provided in tensorflow. (GradientDescent, Adam, RMSProp, etc.)\n* learning rate : It indicates the degree of descending size.\n\n![Imgur](https://pbs.twimg.com/media/DK26ibcXUAEOwel.jpg)\n\n"},{"metadata":{"_cell_guid":"aadf51d1-b5dc-49f5-991c-45ca6ae0ecb2","collapsed":true,"_uuid":"eebe65b3e3633ec738b05288d549ec9e4d13014c","trusted":false},"cell_type":"code","source":"train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"940eb4ef-6f9e-41ed-b692-c4d1d244dfec","_uuid":"df417da0731dabafabe0119275f83d7cb1126641"},"cell_type":"markdown","source":"## 6-6) Compare : original vs. prediction"},{"metadata":{"_cell_guid":"4984c760-fb34-45cf-82d6-6862438ab466","collapsed":true,"_uuid":"26d175949daff82bd0a08115dd8600b1976f795e","trusted":false},"cell_type":"code","source":"prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\ncorrect_prediction = tf.equal(prediction, Y)\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"84fdb2ef-75a6-462c-8688-146e546554a9","_uuid":"ebec32f59831eb056d33d9231d3310122e4a82bb"},"cell_type":"markdown","source":"## 6-7) Activate Model"},{"metadata":{"_cell_guid":"b49eb9b5-3381-4fae-a042-d0ca1d722e3a","collapsed":true,"_uuid":"05ecab0b5cc0c38ec015c2e5d66f9901b71ca463","trusted":false},"cell_type":"code","source":"with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for step in range(10001):\n        sess.run(train, feed_dict={X: train_x, Y: train_y})\n        if step % 1000 == 0:\n            loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n            \n    train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n    test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n    print(\"Model Prediction =\", train_acc)\n    print(\"Test Prediction =\", test_acc)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"730a62f0-ff6f-4c85-b8d3-d2d75c98595f","_uuid":"7cfedba0c43751afcc1c3b783307ea3a3e1d7734"},"cell_type":"markdown","source":"---\n\n# 7. ANN Model Summary & Compare\n## 7-1) ANN - SLP Model\n* train_x, test_x : normalization data\n* 30 features\n* train_y, test_y"},{"metadata":{"_cell_guid":"64fe8680-42d7-463e-a6e9-12a9bb0cb48e","collapsed":true,"_uuid":"bc07298da8f7fca38547a9ac23a7854e80fe390e","trusted":false},"cell_type":"code","source":"def ann_slp():\n    print(\"===========Data Summary===========\")\n    print(\"Training Data :\", train_x.shape)\n    print(\"Testing Data :\", test_x.shape)\n\n    X = tf.placeholder(tf.float32, [None,30])\n    Y = tf.placeholder(tf.float32, [None, 1])\n\n    W = tf.Variable(tf.random_normal([30,1], seed=0), name='weight')\n    b = tf.Variable(tf.random_normal([1], seed=0), name='bias')\n\n    logits = tf.matmul(X,W) + b\n    hypothesis = tf.nn.sigmoid(logits)\n    \n    cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n    cost = tf.reduce_mean(cost_i)\n\n    train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n    correct_prediction = tf.equal(prediction, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n\n    print(\"\\n============Processing============\")\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for step in range(10001):\n            sess.run(train, feed_dict={X: train_x, Y: train_y})\n            if step % 1000 == 0:\n                loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n                print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n\n        train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n        test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n        \n        print(\"\\n============Results============\")\n        print(\"Model Prediction =\", train_acc)\n        print(\"Test Prediction =\", test_acc)\n        \n        return train_acc,test_acc\n    \nann_slp_train_acc, ann_slp_test_acc = ann_slp()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a79243a1-2d1c-48bc-8dca-6cb767ea96ab","_uuid":"2d2f7824ba47035a65ad3024a912f8ac6be3e65d"},"cell_type":"markdown","source":"## 7-2) ANN - SLP - PCA Model\n* pca_train_x, pca_test_x : normalization, PCA\n* 30 -> 10 features\n* train_y, test_y : we can use the same data as above activation values, since there are no changes in y_data"},{"metadata":{"_cell_guid":"b6aae88b-c03c-49f9-867f-c29e218499cd","collapsed":true,"_uuid":"c19d3c5ff404d1ac5d20dbe05322adfaf103f6e0","trusted":false},"cell_type":"code","source":"def ann_slp_pca():\n    sklearn_pca = sklearnPCA(n_components=10)\n\n    print(\"===========Data Summary===========\")\n    pca_train_x = sklearn_pca.fit_transform(train_x)\n    print(\"PCA Training Data :\", pca_train_x.shape)\n\n    pca_test_x = sklearn_pca.fit_transform(test_x)\n    print(\"PCA Testing Data :\", pca_test_x.shape)\n\n    X = tf.placeholder(tf.float32, [None,10])\n    Y = tf.placeholder(tf.float32, [None, 1])\n\n    W = tf.Variable(tf.random_normal([10,1], seed=0), name='weight')\n    b = tf.Variable(tf.random_normal([1], seed=0), name='bias')\n\n    logits = tf.matmul(X,W) + b\n    hypothesis = tf.nn.sigmoid(logits)\n\n    cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n    cost = tf.reduce_mean(cost_i)\n\n    train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n\n    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n    correct_prediction = tf.equal(prediction, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n\n    print(\"\\n============Processing============\")\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for step in range(10001):\n            sess.run(train, feed_dict={X: pca_train_x, Y: train_y})\n            if step % 1000 == 0:\n                loss, acc = sess.run([cost, accuracy], feed_dict={X: pca_train_x, Y: train_y})\n                print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n\n        train_acc = sess.run(accuracy, feed_dict={X: pca_train_x, Y: train_y})\n        test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: pca_test_x, Y: test_y})\n        \n        print(\"\\n============Results============\")\n        print(\"PCA Model Prediction =\", train_acc)\n        print(\"PCA Test Prediction =\", test_acc)\n        \n        return train_acc, test_acc\n    \nann_slp_pca_train_acc, ann_slp_pca_test_acc = ann_slp_pca()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"24c5639d-77ba-4a22-add1-a9fa75310d69","_uuid":"f532469f3c320cf774d41972d177bb3d52219b80"},"cell_type":"markdown","source":"## 7-3) ANN - MLP Model\n* train_x, test_x : normalization data\n* 30 features\n* train_y, test_y"},{"metadata":{"_cell_guid":"3fde2a00-71cc-401e-81d5-102916b200f4","collapsed":true,"_uuid":"11280c554427605e611ef9880fef6dc68306e587","trusted":false},"cell_type":"code","source":"def ann_mlp():\n    print(\"===========Data Summary===========\")\n    print(\"Training Data :\", train_x.shape)\n    print(\"Testing Data :\", test_x.shape)\n\n    X = tf.placeholder(tf.float32, [None,30])\n    Y = tf.placeholder(tf.float32, [None, 1])\n\n    # input\n    W1 = tf.Variable(tf.random_normal([30,60], seed=0), name='weight1')\n    b1 = tf.Variable(tf.random_normal([60], seed=0), name='bias1')\n    layer1 = tf.nn.sigmoid(tf.matmul(X,W1) + b1)\n\n    # hidden1\n    W2 = tf.Variable(tf.random_normal([60,60], seed=0), name='weight2')\n    b2 = tf.Variable(tf.random_normal([60], seed=0), name='bias2')\n    layer2 = tf.nn.sigmoid(tf.matmul(layer1,W2) + b2)\n\n    # hidden2\n    W3 = tf.Variable(tf.random_normal([60,90], seed=0), name='weight3')\n    b3 = tf.Variable(tf.random_normal([90], seed=0), name='bias3')\n    layer3 = tf.nn.sigmoid(tf.matmul(layer2,W3) + b3)\n\n    # output\n    W4 = tf.Variable(tf.random_normal([90,1], seed=0), name='weight4')\n    b4 = tf.Variable(tf.random_normal([1], seed=0), name='bias4')\n    logits = tf.matmul(layer3,W4) + b4\n    hypothesis = tf.nn.sigmoid(logits)\n\n    cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n    cost = tf.reduce_mean(cost_i)\n\n    train = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(cost)\n\n    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n    correct_prediction = tf.equal(prediction, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n\n    print(\"\\n============Processing============\")\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for step in range(10001):\n            sess.run(train, feed_dict={X: train_x, Y: train_y})\n            if step % 1000 == 0:\n                loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n                print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n\n        train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n        test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n        \n        print(\"\\n============Results============\")\n        print(\"Model Prediction =\", train_acc)\n        print(\"Test Prediction =\", test_acc)\n        \n        return train_acc,test_acc\n    \nann_mlp_train_acc, ann_mlp_test_acc = ann_mlp()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"30314455-9b3d-44f9-a93c-20d4baa077ba","_uuid":"856f2fb5c12efc001d684524f4be819ce5ba733b"},"cell_type":"markdown","source":"## 7-4) ANN - MLP - PCA Model\n* pca_train_x, pca_test_x : normalization, PCA\n* 30 -> 10 features\n* train_y, test_y : we can use the same data as above activation values, since there are no changes in y_data"},{"metadata":{"_cell_guid":"f0c782ad-7c50-4dfb-b0b2-80f30ff1797e","collapsed":true,"_uuid":"65eea8735248469cd087b3f9b7c8c15e1614a702","trusted":false},"cell_type":"code","source":"def ann_mlp_pca():\n    sklearn_pca = sklearnPCA(n_components=10)\n\n    print(\"===========Data Summary===========\")\n    pca_train_x = sklearn_pca.fit_transform(train_x)\n    print(\"PCA Training Data :\", pca_train_x.shape)\n\n    pca_test_x = sklearn_pca.fit_transform(test_x)\n    print(\"PCA Testing Data :\", pca_test_x.shape)\n\n    X = tf.placeholder(tf.float32, [None,10])\n    Y = tf.placeholder(tf.float32, [None, 1])\n\n    # input\n    W1 = tf.Variable(tf.random_normal([10,64], seed=0), name='weight1')\n    b1 = tf.Variable(tf.random_normal([64], seed=0), name='bias1')\n    layer1 = tf.nn.sigmoid(tf.matmul(X,W1) + b1)\n\n    # hidden1\n    W2 = tf.Variable(tf.random_normal([64,128], seed=0), name='weight2')\n    b2 = tf.Variable(tf.random_normal([128], seed=0), name='bias2')\n    layer2 = tf.nn.sigmoid(tf.matmul(layer1,W2) + b2)\n\n    # hidden2\n    W3 = tf.Variable(tf.random_normal([128,128], seed=0), name='weight3')\n    b3 = tf.Variable(tf.random_normal([128], seed=0), name='bias3')\n    layer3 = tf.nn.sigmoid(tf.matmul(layer2,W3) + b3)\n\n    # output\n    W4 = tf.Variable(tf.random_normal([128,1], seed=0), name='weight4')\n    b4 = tf.Variable(tf.random_normal([1], seed=0), name='bias4')\n    logits = tf.matmul(layer3,W4) + b4\n    hypothesis = tf.nn.sigmoid(logits)\n\n    cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n    cost = tf.reduce_mean(cost_i)\n\n    train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n\n    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n    correct_prediction = tf.equal(prediction, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n\n    print(\"\\n============Processing============\")\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for step in range(10001):\n            sess.run(train, feed_dict={X: pca_train_x, Y: train_y})\n            if step % 1000 == 0:\n                loss, acc = sess.run([cost, accuracy], feed_dict={X: pca_train_x, Y: train_y})\n                print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n\n        train_acc = sess.run(accuracy, feed_dict={X: pca_train_x, Y: train_y})\n        test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: pca_test_x, Y: test_y})\n        \n        print(\"\\n============Results============\")\n        print(\"PCA Model Prediction =\", train_acc)\n        print(\"PCA Test Prediction =\", test_acc)\n        \n        return train_acc,test_acc\n        \nann_mlp_pca_train_acc, ann_mlp_pca_test_acc = ann_mlp_pca()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"087d207b-d75a-47bf-aecd-913153b1c59e","_uuid":"abc120b032cc54ecf5a7e0c31f6a8711cbd8eab8"},"cell_type":"markdown","source":"---\n\n# 8. Show Results"},{"metadata":{"_cell_guid":"e2196f18-ab20-4ff2-bcc9-a4adbb7c329d","collapsed":true,"_uuid":"75cf308f8477274a7fcf822c183ce7b806002378","trusted":false},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['id'] = test_id\nsub['Predict_Type'] = test_predict.astype(int)\nsub['Origin_Type'] = test_y\nsub['Correct'] = test_correct\nsub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"052cbb93-69ff-447d-a7c6-fba5354e1cb0","_uuid":"987dd2e5eb00eb28fd2d489b113aa9f67b5520b4"},"cell_type":"markdown","source":"---\n\n# 9. Submission"},{"metadata":{"_cell_guid":"3b7dd25c-c74a-4ce5-9758-cbe85980acb2","collapsed":true,"_uuid":"3bf8db7f51624347b485a3804622dd1a7ae2ae3b","trusted":false},"cell_type":"code","source":"sub[['id','Predict_Type']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d67af6b4-c85b-4d31-a465-ae60ca7d8d61","_uuid":"e4d3d7cefbb19a896c1a7355974aa6eb841be296"},"cell_type":"markdown","source":"---\n\n# 10. Conclusion\nYou can make your own ANN model with modifying **learning_rate, step range**.\n\n\nPlanning : ANN-SLP with PCA, ANN-MLP\n\nWant to see my another kernels?\n\n* **Linear Regression(R ver.) [ How much will the premium be?](https://www.kaggle.com/mirichoi0218/regression-how-much-will-the-premium-be)**\n\nUpvotes and Comments are fully Welcomed :-)\n\nThank you for watching!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}